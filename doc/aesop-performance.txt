= Aesop Performance Analysis

== Introduction

This document examines the performance of Aesop in terms of runtime
performance, memory efficiency, programmer productivity, and compile time
performance.  

Aesop is a programming language and programming model designed
to implement distributed system software with high development productivity and run
time efficiency.
Further details about the languange and development environment can be found
in the Aesop User's Guide.

The remainder of this document is organized as follows.
<<sec-case-study>> describe a network service
case study which is then used to evaluate the performance of Aesop
relative to more traditonal server architectures in terms of performance
(<<runtime-perf>>), memory usage (<<sec-memory>>), and productivity
(<<sec-productivity>>).  <<sec-overhead-analyis>> provides a more detailed
breakdown of specific sources of Aesop overhead, while 
<<sec-compile>> concludes by discussing Aesop compile-time code translation
performance.

[[sec-case-study]]
== Case study description

We will use a simple network serivce case study for quantitative
evaluation of the Aesop programming language.  The case study is a TCP
server that can write or read data from local files.  It is expected to
process requests from multiple clients simultaneously.  The description
of the test server and client are given in the following subsections.

[[sec-client-design]]
=== Client Design

Each of the example servers used for comparison in this document implement
an identical request protocol and are therefore evaluated using the same
client test harness.  The client is a basic C program that uses TCP sockets to send messages to
the server.  It use MPI to coordinate processes and generate a highly
concurrent workload.

The client will execute in a loop generating a specifed number of
operations to the server. The general flow is that each client process sends a request
to the server that contains an optional payload. The client then waits for
the server to send an acknowledgement that also contain an optional payload.

==== Request Types

The client supports the following request types.

===== Read

The client sends a request with a file name and a size. The server will then
open the file, read the contents up to the size specified. The server returns
the data with the acknowledgement of the operation.

===== Write

The client sends a request with a file name, size and payload. The server will
then create the file and write the payload. The server then sends an
acknowledgement to client.

===== Read-Null

This request is identical to the *Read* request, except that the server
sends uninitialized data rather than performing any file I/O.
The client sends a request with a size to the server. The server then allocates
a buffer for the response based on the size the client requested. The server
then sends an acknowledgement with this buffer as the payload.

===== Write-Null

This request is identical to the *Write* request, except that the server
discards incoming data rather than performing any file I/O.
The client sends a request with a size and a payload. The server recieves 
the request but then simply discards the payload. The server sends an 
acknowledgement back to the client.

[float]
==== Implementation

The client test harness provides command line parameters to control the
request type, the number of requets, and the size of the request. Each
process barriers until they are all ready to
connect to the server. Clients then exit the barrier, connect to the server,
and begin sending requests in a loop. Once each process completes its
requests, it waits
at another barrier and then reports statistics about the run.  Each process
records the total amount of time taken to execute its workload (beginning
before the initial connection and ending after receipt of the last
acknowledgement).  The time taken by the slowest process is reported as the 
aggregate run time.  Each process also records the time needed to service each
individual request (from before the request is sent until after the
acknowledgement is received) in order to calculate statistics about
individual request latencies.  

=== Server Design

The server supports the client request types listed above. Each server
implementation accepts client connections and then waits for requests on
those connections.  The following diagram illustrates the steps performed by
each request type.

.Server Request Handling
[graphviz]
--------
digraph G
{
   subgraph I {
  rank = same;
   incoming [label="client connects", shape="box"];
   close [label="close connection"];
   }

   wait [label="receive request"];

   incoming -> wait;
   wait -> close [label="client closes connection"];
   wait -> read_1 [label = "READ"];
   wait -> readn_1 [label = "READ NULL"];
   wait -> write_1 [label = "WRITE"];
   wait -> writen_1 [label = "WRITE NULL"];

   subgraph R {
   read_1 [label="read from file"];
   read_2 [label="send data"];
   read_1 -> read_2 -> wait;
   }

   subgraph W {
   write_1 [label="receive data"];
   write_2 [label="write to file"];
   write_1 -> write_2 -> wait;
   }

   subgraph RN {
   readn_1 [label="send data"];
   readn_1 -> wait;
   }
   
   subgraph WN {
   writen_1 [label="receive data"];
   writen_1 -> wait;
   }
}
--------

We implemented the same request protocol in multiple server daemons
in order to contrast different approaches to concurrent
request processing.  Each server uses the same fundamental coding style to 
the degree possible.  One server is implemented using the Aesop language,
while all other servers are implemented in C.  The `pthread` library was
used in all cases that required explicit threading, while the `libev`
library was used in all cases that required an explicit event loop
<<libev>>.  

==== Server Implementations

We implemented six server types including Aesop. Each server type examines
a different type of concurrency model.
 
===== Aesop

The Aesop server is implemented in the Aesop programming language. The server uses a
'lonely pbranch' to service each client. All operations for a client are
handled within a single pbranch.  The socket and file operations are
performed with blocking Aesop functions that are provided by the Aesop
standard library.  The underlying socket resource uses
non-blocking sockets with a thread pool of 12 threads. The file resource uses
synchronous IO and a thread pool with 4 threads.

===== Thread-per-client

The thread-per-client server spawns a thread for each client connection. All
requests for a given client are handled within the same thread. This model uses
blocking socket operations and standard file read and write operations.
Each thread executes until the corresponding client disconnects. 

===== Thread-per-client-nb

The thread-per-client-nb server is identical to the thread-per-client
server, except that is uses non-blocking socket calls in place of blocking
socket calls.  For example, in order to send a message, a thread will
perform non-blocking sends until it encounters the EWOULDBLOCK error code.
It then polls the socket until it is ready and continues sending data.
We implemented this version to investigate the possible performance difference
between the synchronous and asynchronous socket calls in a scenario where
all other factors are held constant.

===== Thread-per-operation
The thread-per-operation server uses an event loop to watch all sockets for
activity.  When a new request is available, a thread is spawned and the request
is handled completely from within that thread. When the request is complete
the thread is destroyed. Blocking socket operations and standard file read
and write functions are used in this implementation.

===== Thead-pool
The thread-pool server uses an event loop to watch all sockets for activity.
When a new request is available, the event loop puts the request on a queue and
wakes up a thread from the thread pool. The request is handled completely
from within a single thread of the thread pool. Blocking sockets and
standard file read and write functions are used in this implementation.  The
thread pool was implemented following best practices for scalable condition
variable performance as described in <<hp-cond-variable>>.

===== Event
The event server uses an event loop not only to detect incoming requests,
but to service them as well.  Each request processing step is executed from
an event loop callback function.  The
event server uses non-blocking sockets and asynchronous file I/O. Note that
although this implementation does not use any explicit threads, the
operating system can still use multiple cores to drive both the
network and disk.

[[runtime-perf]]
== Runtime Performance Evaluation and Analysis

The evaluation of runtime performance was performed by executing a series
of tests using an identical client test harness (described in
<<sec-client-design>>) for each server implementation.  We used small
request sizes in order to stress concurrent, latency-bound requests in an
effort to highlight the ability of each server to quickly multiplex between
small operations.  The read and write tests used the O_DIRECT file access
mode in order to bypass the Linux buffer cache and insure that the disk is
involved in each I/O transfer.   The read-null and write-null tests are
included to stress the pure network performance of each server and insure
that disk I/O is _not_ involved in those cases.

=== Experiment

==== Experiment System

All experiments were executed on the Fusion cluster managed by 
the Argonne Laboratory Computing Resource Center
(LCRF). Fusion is a IBM iDataPlex dx360 M2 system. It features 320 compute
nodes which each consist of two Intel Nehalem 2.6 GH Xeon processors and 36 GB
of RAM. The compute nodes have hyper threading disabled. The cluster has
an Infiniband QDR interconnect. Each compute node also a single SATA 7200 RPM
hard disk for local scratch storage.

==== Experiment Details

The tests were performed by
submitting one job for each request type (read, write, read-null, or
write-null).  Each job executed a series of scripts that launched each
server implementation in turn to service workloads that scaled 
the number of clients from
16 to 1024.  This methodology insured that exactly the same clients and
server nodes were used used when making comparisons across implementations
for a given workload.   Fusion is a shared resource and may experience
increased network contention at times.  We therefore executed each
test case five times and cycled between server implementations in a
round-robin fashion to minimize the possibility of any given server
execution being unfairly penalized by external contention.  We show the
median result in all graphs unless
otherwise noted.  The server daemon was restarted on each
iteration for two reasons.  First, this approach allowed us to capture memory statistics independently for each
run and insure that results were not affected by resources left over from
previous runs.  Secondly, it allowed us to cycle between servers as
described above without any risk of one daemon interfering with the
performance of another daemon.

On fusion we determined that we could use 16 client processes per physical node. Using
more clients per node caused the bottleneck of the test to become the client
nodes instead of the server.  Therefore, for the largest scale tests shown
in this study we utilized 65 total nodes.  One node acted as the server,
while 64 nodes executed up to a total of 1024 client processes.  When scaling the
number of client processes, we also scaled the number of physical client
nodes in the same manner.  Thus the 16 process case used one physical client
node, the 128 process case used 8 physical client nodes, and so on.

The clients connected to the server using the IB network with IPoIB.  Note
that MPI is only used in the client test harness for coordination and timing
among client processes.  All communication between clients and servers is
performed with TCP/IP sockets.

===== Read

The read test had clients each issue 16 requests asking for 4 KiB from
disk. Each client specifies a unique file to be read on on each request. All
clients specifiy unique files. The files are first generated by a script
that runs before the read test starts. The script generates files for every
client in the local storage of the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 16 --size 4096 --type-r
*********************

===== Write

The write test had clients each issue 16 requests sending 4 KiB of data to
be written to disk. Each client specifies a unique file name for each request
and all clients specifiy unique files from each other. The directory
containing all the files is deleted between each test iteration.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 16 --size 4096 --type-w
*********************

===== Read-Null

The read-null test had clients each issue 4096 requests requesting 4 KiB of
data to be returned from the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 4096 --size 4096 --type-rn
*********************

===== Write-Null

The write-null test had clients each issue 4096 requests sending 4 KiB of 
data to be discarded by the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 4096 --size 4096 --type-wn
*********************

=== Expectations

Aesop is intended to improve developer productivity while preserving
scalable performance for system software workloads.  The goal of the raw
performance comparison therefore is not to show that Aesop offers superior
performance to traditional approaches in all scenarios.  Instead, we want to
determine if Aesop offers sufficient performance for consideration
as a viable programming model based on its productivity
merits.  We will elaborate on the productivity aspect of the comparison later in
<<sec-productivity>>.

=== Evaluation

Here the runtime results are presented from the experimentation. All graphs
are shown in log scale.

==== Disk I/O

.Runtime Performance for Write Test
[[fig-writehist]]
image::fig/write-hist.png[]

<<fig-writehist>> shows the overall run time of the concurrent write
workload for each server implementation as the number of client processes
is scaled from 16 to 1024.  In this graph we see that Aesop does not
perform as well as the other servers for small workloads (taking 3.2
seconds at the smallest scale, verses 1.9 seconds for the thread-per-op
server).  However, Aesop is the fastest server at the largest scale
(taking 119.8 seconds verses 130.8 seconds for the nearest competitors
in thread-per-client and threhad-per-client-nb).  

.Runtime Performance for Read Test
[[fig-readhist]]
image::fig/read-hist.png[]

<<fig-readhist>> shows the results of the read experiment.  Aesop performs
more favorably at small scale for this workload than in the previously shown
write workload.  At the largest scale,
Aesop completes the test in 48.3 seconds verses 46.1 seconds for the fastest
server (thread-pool).  The event server performs particularly poorly in all
cases, ultimately running the largest scale test in 77.2 seconds.

The small scale results for Aesop may indicate that additional tuning
is needed to improve latency for small test runs.  The issue is likely
isolated to the write path of the file I/O resource in the Aesop standard
library, as we see assymetric results in the read and write tests for Aesop
in terms of its relative performance.

==== Network I/O

The write-null and read-null experiments were conducted in the same manner
as the write and read tests.  The difference in this case is that no disk
access was perfomed.  In the write case, incoming data was discarded by the
server.  In the read case, the server transmitted uninitialized data.

.Runtime Performance for Write-Null Test
[[fig-writenull]]
image::fig/write-null-hist.png[]

.Runtime Performance for Read-Null Test
[[fig-readnull]]
image::fig/read-null-hist.png[]

Aesop is competitive with (and in most cases better than) the other
implementations except for the thread-per-client server in both the
write-null (<<fig-writenull>>) and read-null (<<fig-readnull>>) evaluation.  We were unable to
isolate a concrete reason for this discrepancy in profiling.  One notable
difference in the two implementations, however, is that the
thread-per-client server uses blocking socket operations, while the aesop
socket resource uses non-blocking operations.  Based on this observation, we implemented the
thread-per-client-nb server to isolate the impact of non-blocking socket
operations on performance.  The thread-per-client-nb implementation is
identical to the thread-per-client implementation except that each socket
uses non-blocking operations and polling to transmit and receive data.  As
seen in these tests, the use of non-blocking operations slows down the
thread-per-client server to the point that it is practically equivalent to
the Aesop server at scale.

Another notable observation in these graphs is that the Aesop server is
competative at small scale, and in fact is the fastest implementation in the
16 client process read-null test and nearly the fastest in the 16 client
process write-null test.  This supports the observation from the previous
section that poor Aesop performance at small scale is likely a tuning flaw
in the file resource used in the Aesop standard library, rather than a
fundamental programming language problem.

==== Fairness

This section evaluates the fairness of the various server implementations in
terms of service times across clients.  We illustrate the fairness of each
server by plotting the difference between the fastest and slowest total time
for a client to complete all of its requests.
The fastest client is the bottom of the bar and the slowest is the top of bar.
These results are from the 1024 client size using the same iteration that
was selected for the maximum runtime performance above.

.Fastest and Slowest Total Client Runtime for Write Test
[[fig-writetime]]
image::fig/write-time.png[]

.Fastest and Slowest Total Client Runtime for Read Test
[[fig-readtime]]
image::fig/read-time.png[]

In <<fig-readtime>> and <<fig-writetime>> we see that the event server is
the only fair server. However, it achieves this fairness by sacrificing
overall throughput.  During our initial experimentation, aesop was configured
to be a fair system as it favored executing all disk and network operations
in FIFO order if possible.  We discovered that this behavior was detrimental
to throughput in practice; the server performed better if it made progress
on requests that had most recently completed a network or disk operation
rather than requests that had been waiting the longest.  We observed the
same behavior in the thread-pool server as well and made the same adjustment
for it to favor "hot" requests as opposed to FIFO ordering.

Interestingly, we see that the thread-per-client, thread-per-client-nb, and
thread-per-op servers are more fair on the write test than on the read test.

.Fastest and Slowest Total Client Runtime for Read-Null Test
[[fig-readnulltime]]
image::fig/read-null-time.png[]

.Fastest and Slowest Total Client Runtime for Write-Null Test
[[fig-writenulltime]]
image::fig/write-null-time.png[]

The <<fig-readnulltime>> and <<fig-writenulltime>> show similar fairness between
all server types except thread-pool in the case of the write-null test case.
The thread-pool server is the only one that shows a notable difference in
fairness due to its scheduling approach.

==== Latency

.Per Operation Latency Write Test (Min/Max/First Quartile/Third Quartile)
[[fig-writelat]]
image::fig/write-lat.png[]

.Per Operation Latency Read Test (Min/Max/First Quartile/Third Quartile)
[[fig-readlat]]
image::fig/read-lat.png[]

The last client metric is an examination of per operation latency. The
client measures the latency of each individual request and the computes the minimum and
maximum latency, the first quartile latency and third quartile latency. These
metrics are graphed using a box and whiskers plot. The box represents the
first and third quartiles and the whiskers are the minimum and maximum
values. The following results are for the 1024 client size selected from
the same iteration as the maximum runtime graphs. <<fig-readlat>> and
<<fig-writelat>> show the aesop offers vary comparitve latency performance
as the other configurations and only noteably thread-per-op and event are
signficantly worse.  An interesting observation is that the overall fairness
across clients shown in the previous section does not appear to correlate in
any way with individual request latency.

.Per Operation Latency Write-Null Test (Min/Max/First Quartile/Third Quartile)
[[fig-writenulllat]]
image::fig/write-null-lat.png[]

.Per Operation Latency Read-Null Test (Min/Max/First Quartile/Third Quartile)
[[fig-readnulllat]]
image::fig/read-null-lat.png[]

In <<fig-readnulllat>> and <<fig-writenulllat>> we see again that aesop has
very good latency metrics compared to the other servers. In this case the
thread-per-op and event are noticeably worse than the other server
implementations.

==== Summary

The runtime analysis shows that aesop is competitive with the most
optimal alternate server implementations at the 1024 client size.
In the _read_ test, aesop is 4.5% slower that the fastest implementation,
threadpool. Aesop was the fastest implementation in the _write_ test.
The thread-per-client implementation was fastest in the _read null_ and
_write null_ tests.  Here aesop is significantly slower at around 25-30%,
however, as we showed above, the difference in performance was due to the
native performance difference in synchronous and asynchronous sockets.
If we look at the performance comparison to the thread-per-client-nb, aesop
was only 4.7% slower in the _read null_ test and was faster in the _write null_
test. If aesop were using a native asynchronous transport such as Infiniband verbs
or SSM, we believe that it would perform as well as a thread-per-client implementation.

Another interesting factor was that there was no one fastest server
implementation. Each workload presents a different challenge and building
a single tuned server is difficult. A key advantage of aesop is the ability
to change the underyling implementation/turning for resources or the aesop
runtime without changing the aesop server source. During our investigation
we experimented with different underlying thread models for the network and
file resources of aesop. The aesop resources can be configured at runtime
to use the different thread models or tuning options. With the knowledge
that building a specific server implementation that is optimal for a generic
workload, aesop becomes powerful because these types of changes can be made
without ever changing the server source.

[[sec-memory]]
== Runtime Memory Efficiency

Another aspect of the overall performance is the memory efficiency of each
server implementation.  In this section we compare aesop to the other server implementations
as we did for the runtime performance.

=== Experiment

During the experiment detailed in the <<runtime-perf>> section. We recorded
the VmHWM stat from the server when the client test was completed. The VmHWM
stat is recorded by Linux during an applications runtime and represents the
peak resident set size (RSS). RSS represents the amount of paged-in memory.
If we looked at the VmPeak which is the total required virtual memory, this
would punish the models which use numerous threads.

=== Evaluation

The following graphs show the memory usage in KiB in log scale. In
<<fig-readmem>> and <<fig-writemem>> we see that thread-pool limits the
memory usage as the client work load increases because the thread-pool 
by design limits the number of requests that can be in progress at once. The
other server implementations scale as the number of clients increase.

<<fig-readnullmem>> and <<fig-writenullmem>> show a similar result as the
disk I/O tests. Aesop demonstrates that it is no worse then any of the thread
models.

.Memory Usage Read Test 
[[fig-readmem]]
image::fig/read-mem.png[]

.Memory Usage Write Test 
[[fig-writemem]]
image::fig/write-mem.png[]

.Memory Usage Read-Null Test 
[[fig-readnullmem]]
image::fig/read-null-mem.png[]

.Memory Usage Write-Null Test 
[[fig-writenullmem]]
image::fig/write-null-mem.png[]

=== Analysis

Aesop does not exhibit worse scaling in terms of memory usage than any of
the other thread-per models but it does add a cost in memory overhead for
each blocking call.

[[sec-productivity]]
== Productivity

The core design element of aesop is to make programming of a concurrent
server easier, so the trade off for memory and runtime performance should
be worth it. To evaluate this we examine the code complexity of each of
server implementations. 

.Implementation complexity analysis.
[[table-complex]]
[cols="3,1,1,1", options="header"]
|============================
| Server Implementation | CC | Mod. CC | SLOC 
| aesop | 16 | 11 | 179 
| thread-per-client | 17 | 12 | 182
| thread-per-client-nb | 17 | 12 | 184
| thread-per-op | 22 | 17 | 249
| thread-pool | 32 | 26 | 313
| event | 28 | 23 | 341
|============================

<<table-complex>> compares the code complexity of each server
implementation using McCabe Cyclomatic Complexity (CC) <<McCabe>>,
Modified McCabe Cyclomatic Complexity (Mod. CC), and Source Lines of Code
(SLOC).  The CC and Mod. CC metrics were measured using the pmccabe tool,
version 2.6, created by Paul Bame <<Bame>>, while the SLOC metrics were
measured using the sloccount tool, version 2.26,
created by David A. Wheeler <<Wheeler>>.

To simplify the comparison, error handling was excluded for all servers
except for assertions on expected return codes.  The protocol definition
(ie, request and acknowledgement structs) as well as helper functions to
loop over send and receive were not counted in any of the implementations,
as these were similar in all four.  Also note that the Aesop version
does not include the Aesop standard library, which 
provides a binding between Aesop and the standard POSIX socket API as part
of its default functionality.  The focus
of this comparison is on the core logic defining the server implementation.

The Aesop and thread-per-client servers are very similar in terms of
complexity. The slight increase in complexity for the thread-per-client
server results from
the additional function calls needed to create and join threads.  The
nonblocking version of the thread-per-client server (thread-per-client-nb)
uses two additional lines of code to place each socket into non-blocking
mode.  The remaining code logic needed to manage the nonblocking socket
calls is implemented in helper functions which are not included in the
analysis.

The thread-pool and event models are both much more complex than the
thread-per-client or aesop model.  In the case of the thread-pool server,
this additional complexity arises from not only the queueing and thread
management logic, but also the event loop which is necessary to detect
incoming requests and dispatch them to the queue.  The event server
complexity arises from the necessity of dividing servicing routines
into multiple sub-functions and manually tracking state between
those functions.  An additional complexity of the event
model which is not captured by these metrics is the fact that control flow
is not preserved across the processing of a given request.  For example,
servicing a write operation requires 5 disconnected event handlers.
Although the event model appears less complex than the thread-pool model according to CC and
Mod. CC, qualitatively it is significantly more challenging to develop.

[[sec-overhead-analysis]]
== Sources of Aesop overhead

As we've seen aesop compares favorably to other concurrency models but loses
some performance compared to best case hand tuned version. Here we examine
the overheads associated with aesop. The first item to examine is the
cost associated with an aesop blocking call.

=== Performance Implications of Blocking Calls

While this isn't immediately visible from looking at the aesop source code,
blocking calls, when compared to a plain C function call, have extra overhead
due to the way they are transformed by the aesop compiler. The following
section highlights the sources of this overhead.

==== State Management

Most of the overhead is caused by the need to preserve the state
of the blocking call while execution (temporarily) switches to another
function. In order to preserve this state, the aesop compiler relocates all
function-scoped variables from the stack to the heap.
When entering a blocking function, heap memory needs to be allocated for these
variables. As allocating heap memory is much more time consuming than
allocating space on the stack, calling a blocking function is more expensive
than calling a regular function.

==== Synchronization Overhead

A second source of overhead originates from the multi-threaded nature of aesop
code. While aesop does not create any threads, many of the aesop resources
internally use threads.  Therefore, the aesop compiler has to ensure that the
emitted code is multi-thread safe. For example, in the following code,
two pbranches might be executing concurrently using different threads.
Since these threads could both by modifying the pwait state simultaneously,
aesop
uses locks to serialize their access. Locks, and other synchronization
primitives account for most of the remaining performance difference between
blocking and regular function calls.

.pwait synchronization
[source,C]
----
pwait {
  pbranch {
     ...
  }
  pbranch {
     ...
  }
}
----

[NOTE]
Currently, the aesop compiler uses a combination of atomic operations and
mutexes to maintain thread-safety. There is an ongoing effort to convert to
atomic operations where possible.

==== Quantifying Blocking Call Overhead

For this test, a regular C function and a aesop blocking function are
called in a loop.  By timing the total time required to complete the loop,
an estimate of the time needed to execute the function call is obtained.

The results were obtained on an intel i7 CPU running at 2.7GHz,
using gcc 4.5.3 (using +-O2+), glibc 2.13-r4 and kernel 3.2.5.

There are a number of different test configurations:

.Test Results
[cols="h,^,^,^,^,^,<,<",valign="middle",frame="topbot",options="header"]
|=====
1.2+<.^| Test   5+| Options                              2+^.^| seconds/call  
          ^d| regular | blocking | malloc/free | mutex | opa   | malloc   | tcmalloc 
| 1        |   X     |          |             |       |       | 2.24e-09 | 2.21e-09
| 2        |         |    X     |             |       |       | 5.54e-08 | 3.62e-08
| 3        |   X     |          |     X       |       |       | 2.34e-08 | 1.62e-08
| 4        |   X     |          |     X       |   X   |       | 3.78e-08 | 2.96e-08
| 5        |   X     |          |     X       |       |   X   | 2.95e-08 | 2.02e-08
|=====

For test 1, a simple regular C function (i.e. not using `__blocking`) taking 2
arguments is used.  Test 2 uses the same function, but this time the
function is marked as `__blocking`.

Tests 3-5 were added to provide a better context for understanding the
magnitude of the blocking call overhead.  For test 3, the function from test 1
was taken but in the function body a single call to +malloc+ and +free+ was
added.  Test 4 is the same as test 3, but also adds a call to lock and unlock
a mutex.  Test 5 replaces the mutex by a single atomic operation
(compare-and-swap).

As a way to study the effect of the malloc implementation, these tests were
also executed using a +tcmalloc+, an alternative memory allocator library.
The results for these are shown in the +tcmalloc+ column.

[NOTE]
The progam used to obtain these results is in the repository:
+tests/blocking-overhead.ae+.

=== Function arguments and stack variables of blocking calls

Aesop, in order to implement the additional functionality provided by blocking
calls, rewrites blocking calls when translating the aesop code to C code.
This translation introduces a certain amount of overhead, both in memory usage
and execution performance. 

The main memory overhead incurred by blocking functions originates from the
need to protect the logical state of the function while temporarily switching
to other functions.

For example, stack variables are moved to the heap. As long as the blocking
function does not complete, the memory for these variables is not released.
Arguments to the function need to be relocated to the heap as well,
and so does the type returned from the function (if not void).

In a normal C program, the items listed above consume stack space. In blocking
functions, these consume heap space instead.  In addition, aesop internally
maintains a number of control structures. Pointers to these structures are
passed as function arguments to the blocking function, and consequently
consume stack space. Currently, aesop adds about 4 pointers and 2 integers to
each blocking function call.

=== Lonely pbranches

A lonely pbranch will keep the enclosing scope alive (up to the function
scope) until the pbranch exits.

.lonely pbranch scope
[source, C]
----
__blocking int test (void)
{
   int var[10000];
   pbranch {
      ...
   }
}
----

So, in the example above, even though the test will return without waiting for
the pbranch to complete, its stack variables (`var` in this case) will
consume memory until the pbranch returns.

[[sec-compile]]
== Compile Time Performance

Currently aesop imposes some overhead when compiling aesop source. This is
demomnstrated in a simple micro-benchmark. The test takes an existing aesop
source fill with 4200 lines or source and is about 116 KB in size and compiles
it to an object file. The same source file is then renamed to a .c file and
four +#define+ are added which redefine the aesop keywords to nothing.

The test is executed on a dual processor Intel Xeon E5620 running at 2.4 GHz
with 24 GB of RAM.

.Compile Time Comparison
[[table-compile]]
[width="50%",cols="1,1>", options="header"]
|============================
| Type | Time (seconds)
| aesop | 112.02
| C | 1.22
|============================

The performance penalty is significant but this only effects development. It
can be mitigated by constructing a Makefile that supports parallel make.

== Bibliography

[bibliography]
- [[[McCabe]]] McCabe, T.J. A Complexity Measure. In IEEE Transactions on
  Software Engineering, vol.SE-2, no.4, pp. 308- 320, Dec. 1976.
- [[[Bame]]] Paul Bame.  pmccabe.  http://parisc-linux.org/~bame/pmccabe/
- [[[Wheeler]]] David A. Wheeler.  sloccount.  http://www.dwheeler.com/sloc/
- [[[libev]]] Marc Lehmann. http://software.schmorp.de/pkg/libev.html
- [[[hp-cond-variable]]] Hewlet Packard Development Company L.P..  "Techniques for
  Improving the Scalability of Applications Using POSIX Thread
  Condition Variables."
  http://h21007.www2.hp.com/portal/download/files/unprot/hpux/MakingConditionVariablesPerform.pdf

== Appendix A - Raw Data

.Read Runtime Data
[[table-readhist]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  0.794610 |  0.706736 |  0.775575 |  2.135912 |  0.861456 |  1.284919 
|  128 |  6.740836 |  3.755610 |  3.865248 |  3.395985 |  5.688868 |  9.602547 
|  256 | 12.352416 | 12.936459 | 13.051936 | 12.383918 | 11.434637 | 19.542018 
|  512 | 23.641337 | 27.395763 | 26.318174 | 24.776392 | 22.944439 | 39.058706 
| 1024 | 48.309275 | 54.852327 | 55.658511 | 47.564368 | 46.179671 | 77.216961 
|============================

.Write Runtime Data
[[table-writehist]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
|clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |   3.200944 |   2.306964 |   2.157996 |   1.934971 |   2.684861 |   2.512977 
|  128 |  18.213375 |  14.991499 |  15.802083 |  16.594684 |  18.065047 |  17.075809 
|  256 |  33.515213 |  32.899976 |  32.548540 |  32.662463 |  35.354696 |  33.900419 
|  512 |  67.640246 |  66.168558 |  66.324414 |  67.188763 |  71.184339 |  69.184007 
| 1024 | 119.799388 | 130.895169 | 130.829158 | 140.270790 | 141.208043 | 140.464092 
|============================


.Read-Null Runtime Data
[[table-readnull]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1.122259 |  1.688565 |  1.845739 |  1.511863 |  1.220004 |  1.225304 
|  128 |  3.640566 |  4.558670 |  5.159914 |  9.884832 |  4.002515 |  7.252784 
|  256 |  7.087344 |  5.400444 |  6.604424 | 19.875843 |  7.448785 | 14.500385 
|  512 | 13.661447 | 10.603367 | 12.862459 | 40.884105 | 14.549549 | 28.967834 
| 1024 | 27.013576 | 21.625077 | 25.814349 | 82.675118 | 29.376645 | 50.822263 
|============================

.Write-Null Runtime Data
[[table-writenull]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1.007547 |  0.932891 |  1.012248 |  1.561492 |   1.368119 |  1.372057 
|  128 |  4.844869 |  3.680892 |  4.771543 | 10.240339 |  16.074748 |  7.923767 
|  256 |  9.263294 |  6.997207 |  9.135328 | 20.342675 |  29.219365 | 15.994714 
|  512 | 18.293856 | 14.069904 | 18.382822 | 41.136291 |  59.712499 | 32.640966 
| 1024 | 37.081757 | 28.534744 | 39.618136 | 83.716303 | 116.833184 | 58.592679 
|============================

.Read Time Data
[[table-readtime]]
[cols="1>,1>,1>,1>", options="header"]
|============================
| server | min | max | x
| aesop | 0.041914 | 48.309275 | 1.0
| thread-per-client | 0.055891 | 54.852327 | 2.0
| thread-per-client-nb | 0.067599 | 55.658511 | 3.0
| thread-per-op | 0.667692 | 47.564368 | 4.0
| thread-pool | 0.073988 | 46.179671 | 5.0
| event | 63.155711 | 77.216961 | 6.0
|============================

.Write Time Data
[[table-writetime]]
[cols="1>,1>,1>,1>", options="header"]
|============================
| server | min | max | x
| aesop | 0.415026 | 119.799388 | 1.0
| thread-per-client | 67.926019 | 130.895169 | 2.0
| thread-per-client-nb | 77.042924 | 130.829158 | 3.0
| thread-per-op | 77.181582 | 140.270790 | 4.0
| thread-pool | 0.915993 | 141.208043 | 5.0
| event | 119.316831 | 140.464092 | 6.0
|============================

.Read-Null Time Data
[[table-readnulltime]]
[cols="1>,1>,1>,1>", options="header"]
|============================
| server | min | max | x
| aesop | 16.165011 | 27.013576 | 1.0
| thread-per-client | 9.041217 | 21.625077 | 2.0
| thread-per-client-nb | 15.326102 | 25.814349 | 3.0
| thread-per-op | 63.169438 | 82.675118 | 4.0
| thread-pool | 20.796985 | 29.376645 | 5.0
| event | 35.460415 | 50.154710 | 6.0
|============================

.Write-Null Time Data
[[table-writenulltime]]
[cols="1>,1>,1>,1>", options="header"]
|============================
| server | min | max | x
| aesop | 26.075373 | 37.081757 | 1.0
| thread-per-client | 20.131762 | 28.534744 | 2.0
| thread-per-client-nb | 29.589322 | 39.618136 | 3.0
| thread-per-op | 69.875107 | 83.716303 | 4.0
| thread-pool | 11.849568 | 116.833184 | 5.0
| event | 51.039087 | 58.592679 | 6.0
|============================

.Read Latency Data
[[table-readlat]]
[cols="1>,1>,1>,1>,1>,1>", options="header"]
|============================
| server | first | min | max | third | x
| aesop | 0.000257 | 0.000251 | 0.000268 | 0.000266 | 1.0
| thread-per-client | 0.000250 | 0.000242 | 0.000267 | 0.000265 | 2.0
| thread-per-client-nb | 0.000249 | 0.000242 | 0.000265 | 0.000264 | 3.0
| thread-per-op | 0.000462 | 0.000340 | 0.000495 | 0.000489 | 4.0
| thread-pool | 0.000256 | 0.000239 | 0.000278 | 0.000275 | 5.0
| event | 0.016989 | 0.006868 | 0.034917 | 0.028945 | 6.0
|============================

.Write Latency Data
[[table-writelat]]
[cols="1>,1>,1>,1>,1>,1>", options="header"]
|============================
| server | first | min | max | third | x
| aesop | 0.003235 | 0.002687 | 0.004293 | 0.003931 | 1.0
| thread-per-client | 0.005229 | 0.002843 | 0.008247 | 0.008217 | 2.0
| thread-per-client-nb | 0.004103 | 0.001798 | 0.008219 | 0.008124 | 3.0
| thread-per-op | 0.017350 | 0.011734 | 0.037548 | 0.030839 | 4.0
| thread-pool | 0.003174 | 0.002525 | 0.004169 | 0.003877 | 5.0
| event | 0.045139 | 0.011745 | 0.103348 | 0.103275 | 6.0
|============================

.Read-Null Latency Data
[[table-readnulllat]]
[cols="1>,1>,1>,1>,1>,1>", options="header"]
|============================
| server | first | min | max | third | x
| aesop | 0.000066 | 0.000041 | 0.000083 | 0.000077 | 1.0
| thread-per-client | 0.000062 | 0.000041 | 0.000074 | 0.000070 | 2.0
| thread-per-client-nb | 0.000067 | 0.000044 | 0.000086 | 0.000082 | 3.0
| thread-per-op | 0.000117 | 0.000062 | 0.000331 | 0.000257 | 4.0
| thread-pool | 0.000078 | 0.000048 | 0.000102 | 0.000096 | 5.0
| event | 0.000077 | 0.000044 | 0.000135 | 0.000116 | 6.0
|============================

.Write-Null Latency Data
[[table-writenulllat]]
[cols="1>,1>,1>,1>,1>,1>", options="header"]
|============================
| server | first | min | max | third | x
| aesop | 0.000070 | 0.000040 | 0.000101 | 0.000091 | 1.0
| thread-per-client | 0.000065 | 0.000042 | 0.000079 | 0.000076 | 2.0
| thread-per-client-nb | 0.000064 | 0.000042 | 0.000081 | 0.000077 | 3.0
| thread-per-op | 0.000110 | 0.000062 | 0.000379 | 0.000302 | 4.0
| thread-pool | 0.000049 | 0.000036 | 0.000053 | 0.000052 | 5.0
| event | 0.000074 | 0.000046 | 0.000184 | 0.000133 | 6.0
|============================

.Read Memory Data
[[table-readmem]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1720 |   840 |   864 |   928 |  860 |   984 
|  128 |  2720 |  2748 |  2600 |  2832 | 1040 |  2988 
|  256 |  4700 |  5256 |  5064 |  5076 | 1160 |  5520 
|  512 |  5284 |  9316 |  9564 |  9352 | 1344 |  8428 
| 1024 | 10276 | 18216 | 18668 | 18288 | 1656 | 19248 
|============================

.Write Memory Data
[[table-writemem]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1692 |   844 |   864 |   944 |  876 |   956 
|  128 |  3152 |  2912 |  2852 |  2932 | 1044 |  2080 
|  256 |  3912 |  5436 |  5152 |  5104 | 1192 |  3292 
|  512 |  6764 |  9844 |  9960 |  9760 | 1316 |  5604 
| 1024 | 14700 | 18648 | 18940 | 18920 | 1652 | 19284 
|============================

.Read-Null Memory Data
[[table-readnullmem]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 | 1580 |  836 |  844 |  864 |  920 |  640 
|  128 | 2060 | 1972 | 1788 | 1000 | 1104 |  696 
|  256 | 2244 | 2992 | 2868 | 1000 | 1212 |  768 
|  512 | 2868 | 5200 | 5004 | 1124 | 1388 |  904 
| 1024 | 3900 | 9752 | 9292 | 4948 | 1744 | 1128 
|============================

.Write-Null Memory Data
[[table-writenullmem]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1600 |   856 |   816 |   852 |  916 |  680 
|  128 |  2876 |  2632 |  2652 |  1728 | 1144 |  744 
|  256 |  4588 |  5176 |  4904 |  1036 | 1248 |  808 
|  512 |  8036 |  9728 |  9604 |  7184 | 1436 | 1336 
| 1024 | 14084 | 19060 | 18764 | 11068 | 1808 | 5632 
|============================
