:aesop: Aesop

= {aesop} Performance Analysis

== Introduction

This document examines the performance of {aesop} in terms of runtime
performance, memory efficiency, programmer productivity, and compile time
performance.  

{aesop} is a programming language and programming model designed
to implement distributed system software with high development productivity and run
time efficiency.
Further details about the language and development environment can be found
in the {aesop} User's Guide.

The remainder of this document is organized as follows.
<<sec-case-study>> describe a network service
case study which is then used to evaluate the performance of {aesop}
relative to more traditional server architectures in terms of performance
(<<runtime-perf>>), memory usage (<<sec-memory>>), and productivity
(<<sec-productivity>>).  <<sec-overhead-analysis>> provides a more detailed
breakdown of specific sources of {aesop} overhead, while 
<<sec-compile>> concludes by discussing {aesop} compile-time code translation
performance.

[[sec-case-study]]
== Case study description

We will use a simple network service case study for quantitative
evaluation of the {aesop} programming language.  The case study is a TCP
server that can write or read data from local files.  It is expected to
process requests from multiple clients simultaneously.  The description
of the test server and client are given in the following subsections.

[[sec-client-design]]
=== Client Design

Each of the example servers used for comparison in this document implement
an identical request protocol and are therefore evaluated using the same
client test harness.  The client is a basic C program that uses TCP sockets to send messages to
the server.  It uses MPI to coordinate processes and generate a highly
concurrent workload.

The client will execute in a loop generating a specified number of
operations to the server. The general flow is that each client process sends a request
to the server that contains an optional payload. The client then waits for
the server to send an acknowledgment that also contains an optional payload.

==== Request Types

The client supports the following request types.

===== Read

The client sends a request with a file name and a size. The server will then
open the file, read the contents up to the size specified. The server returns
the data with the acknowledgment of the operation.

===== Write

The client sends a request with a file name, size and payload. The server will
then create the file and write the payload. The server then sends an
acknowledgment to client.

===== Read-Null

This request is identical to the *Read* request, except that the server
sends uninitialized data rather than performing any file I/O.
The client sends a request with a size to the server. The server then allocates
a buffer for the response based on the size the client requested. The server
then sends an acknowledgment with this buffer as the payload.

===== Write-Null

This request is identical to the *Write* request, except that the server
discards incoming data rather than performing any file I/O.
The client sends a request with a size and a payload. The server receives 
the request but then simply discards the payload. The server sends an 
acknowledgment back to the client.

[float]
==== Implementation

The client test harness provides command line parameters to control the
request type, the number of requests, and the size of the request. Each
process barriers until they are all ready to
connect to the server. Clients then exit the barrier, connect to the server,
and begin sending requests in a loop. Once each process completes its
requests, it waits
at another barrier and then reports statistics about the run.  Each process
records the total amount of time taken to execute its workload (beginning
before the initial connection and ending after receipt of the last
acknowledgment).  The time taken by the slowest process is reported as the 
aggregate run time.  Each process also records the time needed to service each
individual request (from before the request is sent until after the
acknowledgment is received) in order to calculate statistics about
individual request latencies.  

=== Server Design

The server supports the client request types listed above. Each server
implementation accepts client connections and then waits for requests on
those connections.  The following diagram illustrates the steps performed by
each request type.

.Server Request Handling
[graphviz]
--------
digraph G
{
   subgraph I {
  rank = same;
   incoming [label="client connects", shape="box"];
   close [label="close connection"];
   }

   wait [label="receive request"];

   incoming -> wait;
   wait -> close [label="client closes connection"];
   wait -> read_1 [label = "READ"];
   wait -> readn_1 [label = "READ NULL"];
   wait -> write_1 [label = "WRITE"];
   wait -> writen_1 [label = "WRITE NULL"];

   subgraph R {
   read_1 [label="read from file"];
   read_2 [label="send data"];
   read_1 -> read_2 -> wait;
   }

   subgraph W {
   write_1 [label="receive data"];
   write_2 [label="write to file"];
   write_1 -> write_2 -> wait;
   }

   subgraph RN {
   readn_1 [label="send data"];
   readn_1 -> wait;
   }
   
   subgraph WN {
   writen_1 [label="receive data"];
   writen_1 -> wait;
   }
}
--------

We implemented the same request protocol in multiple server daemons
in order to contrast different approaches to concurrent
request processing.  Each server uses the same fundamental coding style to 
the degree possible.  One server is implemented using the {aesop} language,
while all other servers are implemented in C.  The `pthread` library was
used in all cases that required explicit threading, while the `libev`
library was used in all cases that required an explicit event loop
<<libev>>.  

==== Server Implementations

We implemented six server types including {aesop}. Each server type examines
a different type of concurrency model.
 
===== {aesop}

The {aesop} server is implemented in the {aesop} programming language. The server uses a
'lonely pbranch' to service each client. All operations for a client are
handled within a single pbranch.  The socket and file operations are
performed with blocking {aesop} functions that are provided by the {aesop}
standard library.  The underlying socket resource uses
non-blocking sockets with a thread pool of 12 threads. The file resource uses
synchronous IO and a thread pool with 4 threads.

===== Thread-per-client

The thread-per-client server spawns a thread for each client connection. All
requests for a given client are handled within the same thread. This model uses
blocking socket operations and standard file read and write operations.
Each thread executes until the corresponding client disconnects. 

===== Thread-per-client-nb

The thread-per-client-nb server is identical to the thread-per-client
server, except that is uses non-blocking socket calls in place of blocking
socket calls.  For example, in order to send a message, a thread will
perform non-blocking sends until it encounters the EWOULDBLOCK error code.
It then polls the socket until it is ready and continues sending data.
We implemented this version to investigate the possible performance difference
between the synchronous and asynchronous socket calls in a scenario where
all other factors are held constant.

===== Thread-per-operation
The thread-per-operation server uses an event loop to watch all sockets for
activity.  When a new request is available, a thread is spawned and the request
is handled completely from within that thread. When the request is complete
the thread is destroyed. Blocking socket operations and standard file read
and write functions are used in this implementation.

===== Thread-pool
The thread-pool server uses an event loop to watch all sockets for activity.
When a new request is available, the event loop puts the request on a queue and
wakes up a thread from the thread pool. The request is handled completely
from within a single thread of the thread pool. Blocking sockets and
standard file read and write functions are used in this implementation.  The
thread pool was implemented following best practices for scalable condition
variable performance as described in <<hp-cond-variable>>.

===== Event
The event server uses an event loop not only to detect incoming requests,
but to service them as well.  Each request processing step is executed from
an event loop callback function.  The
event server uses non-blocking sockets and asynchronous file I/O. Note that
although this implementation does not use any explicit threads, the
operating system can still use multiple cores to drive both the
network and disk.

[[runtime-perf]]
== Runtime Performance Evaluation and Analysis

The evaluation of runtime performance was performed by executing a series
of tests using an identical client test harness (described in
<<sec-client-design>>) for each server implementation.  We used small
request sizes in order to stress concurrent, latency-bound requests in an
effort to highlight the ability of each server to quickly multiplex between
small operations.  The read and write tests used the O_DIRECT file access
mode in order to bypass the Linux buffer cache and insure that the disk is
involved in each I/O transfer.   The read-null and write-null tests are
included to stress the pure network performance of each server and insure
that disk I/O is _not_ involved in those cases.

=== Experiment

This experiment was executed on Feburary 7, 2012 and Feburary 9, 2012.
The experiment was done using the {aesop} version from Triton and the
client code was from the triton-private repository. A patch was applied
to triton to modify queue usage.

* triton ba74000dc0a0aaa35754015734c2944d45ed0daf
* triton-private d0cc70d12fba6edabe7478e21784f91588496bf2
* triton-file-unfair-queue.patch

==== Experiment System

All experiments were executed on the Fusion cluster managed by 
the Argonne Laboratory Computing Resource Center
(LCRC). Fusion is a IBM iDataPlex dx360 M2 system. It features 320 compute
nodes which each consist of two Intel Nehalem 2.6 GHz Xeon processors and 36 GB
of RAM. The compute nodes have hyper threading disabled. The cluster has
an InfiniBand QDR interconnect. Each compute node also a single SATA 7200 RPM
hard disk for local scratch storage.

==== Experiment Details

The tests were performed by
submitting one job for each request type (read, write, read-null, or
write-null).  Each job executed a series of scripts that launched each
server implementation in turn to service workloads that scaled 
the number of clients from
16 to 1024.  This methodology insured that exactly the same clients and
server nodes were used used when making comparisons across implementations
for a given workload.   Fusion is a shared resource and may experience
increased network contention at times.  We therefore executed each
test case five times to minimize the possibility of any given server
execution being unfairly penalized by external contention.  We show the
median result of the five iterations in all graphs unless otherwise noted.  

On Fusion we determined that we could use 16 client processes per physical node. Using
more clients per node caused the bottleneck of the test to become the client
nodes instead of the server.  Therefore, for the largest scale tests shown
in this study we utilized 65 total nodes.  One node acted as the server,
while 64 nodes executed up to a total of 1024 client processes.  When scaling the
number of client processes, we also scaled the number of physical client
nodes in the same manner.  Thus the 16 process case used one physical client
node, the 128 process case used 8 physical client nodes, and so on.

The clients connected to the server using the IB network with IPoIB.  Note
that MPI is only used in the client test harness for coordination and timing
among client processes.  All communication between clients and servers is
performed with TCP/IP sockets.

===== Read

The read test had clients each issue 16 requests asking for 4 KiB from
disk. Each client specifies a unique file to be read on each request. All
clients specify unique files. The files are first generated by a script
that runs before the read test starts. The script generates files for every
client in the local storage of the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 16 --size 4096 --type-r
*********************

===== Write

The write test had clients each issue 16 requests sending 4 KiB of data to
be written to disk. Each client specifies a unique file name for each request
and all clients specify unique files from each other. The directory
containing all the files is deleted between each test iteration.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 16 --size 4096 --type-w
*********************

===== Read-Null

The read-null test had clients each issue 4096 requests requesting 4 KiB of
data to be returned from the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 4096 --size 4096 --type-rn
*********************

===== Write-Null

The write-null test had clients each issue 4096 requests sending 4 KiB of 
data to be discarded by the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 4096 --size 4096 --type-wn
*********************

=== Expectations

{aesop} is intended to improve developer productivity while preserving
scalable performance for system software workloads.  The goal of the raw
performance comparison therefore is not to show that {aesop} offers superior
performance to traditional approaches in all scenarios.  Instead, we want to
determine if {aesop} offers sufficient performance for consideration
as a viable programming model based on its productivity
merits.  We will elaborate on the productivity aspect of the comparison later in
<<sec-productivity>>.

=== Evaluation

Here the runtime results are presented from the experimentation. All graphs
are shown in log scale.

==== Disk I/O

.Runtime Performance for Write Test
[[fig-writehist]]
image::fig/write-hist.png[]

<<fig-writehist>> shows the overall run time of the concurrent write
workload for each server implementation as the number of client processes
is scaled from 16 to 1024.  In this graph we see that {aesop} does not
perform as well as the other servers for small workloads (taking 3.2
seconds at the smallest scale, verses 1.9 seconds for the thread-per-op
server).  However, {aesop} is the fastest server at the largest scale
(taking 119.8 seconds verses 130.8 seconds for the nearest competitors
in thread-per-client and thread-per-client-nb).  

.Runtime Performance for Read Test
[[fig-readhist]]
image::fig/read-hist.png[]

<<fig-readhist>> shows the results of the read experiment.  {aesop} performs
more favorably at small scale for this workload than in the previously shown
write workload.  At the largest scale,
{aesop} completes the test in 48.3 seconds verses 46.1 seconds for the fastest
server (thread-pool).  The event server performs particularly poorly in all
cases, ultimately running the largest scale test in 77.2 seconds.

The small scale results for {aesop} may indicate that additional tuning
is needed to improve latency for small test runs.  The issue is likely
isolated to the write path of the file I/O resource in the {aesop} standard
library, as we see asymmetric results in the read and write tests for {aesop}
in terms of its relative performance.

==== Network I/O

The write-null and read-null experiments were conducted in the same manner
as the write and read tests.  The difference in this case is that no disk
access was performed.  In the write case, incoming data was discarded by the
server.  In the read case, the server transmitted uninitialized data.

.Runtime Performance for Write-Null Test
[[fig-writenull]]
image::fig/write-null-hist.png[]

.Runtime Performance for Read-Null Test
[[fig-readnull]]
image::fig/read-null-hist.png[]

{aesop} is competitive with (and in most cases better than) the other
implementations except for the thread-per-client server in both the
write-null (<<fig-writenull>>) and read-null (<<fig-readnull>>) evaluation.  We were unable to
isolate a concrete reason for this discrepancy in profiling.  One notable
difference in the two implementations, however, is that the
thread-per-client server uses blocking socket operations, while the {aesop}
socket resource uses non-blocking operations.  Based on this observation, we implemented the
thread-per-client-nb server to isolate the impact of non-blocking socket
operations on performance.  The thread-per-client-nb implementation is
identical to the thread-per-client implementation except that each socket
uses non-blocking operations and polling to transmit and receive data.  As
seen in these tests, the use of non-blocking operations slows down the
thread-per-client server to the point that it is practically equivalent to
the {aesop} server at scale.

Another notable observation in these graphs is that the {aesop} server is
competitive at small scale, and in fact is the fastest implementation in the
16 client process read-null test and nearly the fastest in the 16 client
process write-null test.  This supports the observation from the previous
section that poor {aesop} performance at small scale is likely a tuning flaw
in the file resource used in the {aesop} standard library, rather than a
fundamental programming language problem.

==== Fairness

This section evaluates the fairness of the various server implementations in
terms of service times across clients.  We illustrate the fairness of each
server by plotting the difference between the fastest and slowest total time
for a client to complete all of its requests.
The fastest client is the bottom of the bar and the slowest is the top of bar.
These results are from the 1024 client size using the same iteration that
was selected for the maximum runtime performance above.

.Fastest and Slowest Total Client Runtime for Write Test
[[fig-writetime]]
image::fig/write-time.png[]

.Fastest and Slowest Total Client Runtime for Read Test
[[fig-readtime]]
image::fig/read-time.png[]

In <<fig-writetime>> and <<fig-readtime>> we see that the event server is
the only fair server. However, it achieves this fairness by sacrificing
overall throughput.  During our initial experimentation, {aesop} was configured
to be a fair system as it favored executing all disk and network operations
in FIFO order if possible.  We discovered that this behavior was detrimental
to throughput in practice; the server performed better if it made progress
on requests that had most recently completed a network or disk operation
rather than requests that had been waiting the longest.  We observed the
same behavior in the thread-pool server as well and made the same adjustment
for it to favor "hot" requests as opposed to FIFO ordering.

Interestingly, we see that the thread-per-client, thread-per-client-nb, and
thread-per-op servers are more fair on the write test than on the read test.

.Fastest and Slowest Total Client Runtime for Write-Null Test
[[fig-writenulltime]]
image::fig/write-null-time.png[]

.Fastest and Slowest Total Client Runtime for Read-Null Test
[[fig-readnulltime]]
image::fig/read-null-time.png[]

The <<fig-writenulltime>> and <<fig-readnulltime>> show similar fairness between
all server types except thread-pool in the case of the write-null test case.
The thread-pool server is the only one that shows a notable difference in
fairness due to its scheduling approach.

==== Latency

.Per Operation Latency Write Test (Min/Max/First Quartile/Third Quartile)
[[fig-writelat]]
image::fig/write-lat.png[]

.Per Operation Latency Read Test (Min/Max/First Quartile/Third Quartile)
[[fig-readlat]]
image::fig/read-lat.png[]

The last client metric is an examination of per operation latency. The
client measures the latency of each individual request and then computes the minimum and
maximum latency, the first quartile latency and third quartile latency. These
metrics are graphed using a box and whiskers plot. The box represents the
first and third quartiles and the whiskers are the minimum and maximum
values. The following results are for the 1024 client size selected from
the same iteration as the maximum runtime graphs. <<fig-writelat>> and
<<fig-readlat>> show {aesop} offers vary comparative latency performance
as the other configurations and only notably thread-per-op and event are
significantly worse.  An interesting observation is that the overall fairness
across clients shown in the previous section does not appear to correlate in
any way with individual request latency.

.Per Operation Latency Write-Null Test (Min/Max/First Quartile/Third Quartile)
[[fig-writenulllat]]
image::fig/write-null-lat.png[]

.Per Operation Latency Read-Null Test (Min/Max/First Quartile/Third Quartile)
[[fig-readnulllat]]
image::fig/read-null-lat.png[]

In <<fig-writenulllat>> and <<fig-readnulllat>> we see again that {aesop} has
very good latency metrics compared to the other servers. In this case the
thread-per-op and event are noticeably worse than the other server
implementations.

==== Summary

The runtime analysis shows that {aesop} is competitive with the most
optimal alternate server implementations at the 1024 client size.
In the _read_ test, {aesop} is 4.5% slower than the fastest implementation,
thread-pool. {aesop} was the fastest implementation in the _write_ test.
The thread-per-client implementation was fastest in the _read null_ and
_write null_ tests.  Here {aesop} is significantly slower at around 25-30%,
however, as we showed above, the difference in performance was due to the
native performance difference in synchronous and asynchronous sockets.
If we look at the performance comparison to the thread-per-client-nb, {aesop}
was only 4.7% slower in the _read null_ test and was faster in the _write null_
test. If {aesop} were using a native asynchronous transport such as InfiniBand verbs
or SSM, we believe that it would perform as well as a thread-per-client implementation.

Another interesting factor was that there was no one fastest server
implementation. Each workload presents a different challenge and building
a single tuned server is difficult. A key advantage of {aesop} is the ability
to change the underlying implementation or turning for the {aesop}
runtime without changing the server code. During our investigation
we experimented with different underlying thread models for the network and
file resources of {aesop}. The {aesop} resources can be configured at runtime
to use the different thread models or tuning options. With the knowledge
that building a specific server implementation, which is optimal for a generic
workload, is extremely difficult, {aesop} becomes powerful because these types of changes can be made
without changing the server source.

[[sec-memory]]
== Runtime Memory Efficiency

Another aspect of the overall performance is the memory efficiency of each
server implementation.  In this section we compare {aesop} to the other server implementations
using the same runtime performance experitment.

=== Experiment

The memory utilization of each server implementation was captured during
the performance experiments detailed in <<runtime-perf>>.  We recorded
the *VmHWM* stat from the server when the client test was completed. The
*VmHWM* stat is a Linux-specific metric that represents the peak resident
set size (RSS) of an executable, where RSS corresponds to the amount
of paged-in memory used by the executable.

=== Evaluation

The following graphs show the memory usage in KiB in log scale.

.Memory Usage Write Test 
[[fig-writemem]]
image::fig/write-mem.png[]

.Memory Usage Read Test 
[[fig-readmem]]
image::fig/read-mem.png[]

In <<fig-writemem>> and <<fig-readmem>> we see that thread-pool limits the
memory usage as the client work load increases because the thread-pool 
by design limits the number of requests that can be in progress at once. The
other server implementations scale as the number of clients increase.

.Memory Usage Write-Null Test 
[[fig-writenullmem]]
image::fig/write-null-mem.png[]

.Memory Usage Read-Null Test 
[[fig-readnullmem]]
image::fig/read-null-mem.png[]

<<fig-writenullmem>> and <<fig-readnullmem>> show a similar result as the
disk I/O tests.  In this case the event server also produces favorable
results (in addition to the thread pool server) because it is the only
implementation which does not spawn any additional threads to handle
increasing numbers of network connections.

Although the {aesop} server cannot match the thread-pool server in terms of
memory usage, it does compare favorably to the thread-per-client and
thread-per op servers.  In the read test, for example, {aesop} consumes
roughly 10 MiB of paged in memory verses almost 18 MiB of paged in memory
for the thread-per-client server at scale.

Note that the thread-per-client and thread-per-op models consume virtual
memory at a much larger rate due to the number of thread stacks allocated.
We chose not to evaluate this metric, however, as the resident memory seems
to be a more relevant metric in practice.

[[sec-productivity]]
== Productivity

The core design element of {aesop} is to make programming of a concurrent
server easier, so the trade off for memory and runtime performance should
be worth it. To evaluate this we examine the code complexity of each of the
server implementations. 

.Implementation complexity analysis.
[[table-complex]]
[cols="3,1,1,1", options="header"]
|============================
| Server Implementation | CC | Mod. CC | SLOC 
| aesop | 16 | 11 | 179 
| thread-per-client | 17 | 12 | 182
| thread-per-client-nb | 17 | 12 | 184
| thread-per-op | 22 | 17 | 249
| thread-pool | 32 | 26 | 313
| event | 28 | 23 | 341
|============================

<<table-complex>> compares the code complexity of each server
implementation using McCabe Cyclomatic Complexity (CC) <<McCabe>>,
Modified McCabe Cyclomatic Complexity (Mod. CC), and Source Lines of Code
(SLOC).  The CC and Mod. CC metrics were measured using the pmccabe tool,
version 2.6, created by Paul Bame <<Bame>>, while the SLOC metrics were
measured using the sloccount tool, version 2.26,
created by David A. Wheeler <<Wheeler>>.

To simplify the comparison, error handling was excluded for all servers
except for assertions on expected return codes.  The protocol definition
(ie, request and acknowledgment structs) as well as helper functions to
loop over send and receive were not counted in any of the implementations,
as these were similar in all four.  Also note that the {aesop} version
does not include the {aesop} standard library, which 
provides a binding between {aesop} and the standard POSIX socket API as part
of its default functionality.  The focus
of this comparison is on the core logic defining the server implementation.

The {aesop} and thread-per-client servers are very similar in terms of
complexity. The slight increase in complexity for the thread-per-client
server results from
the additional function calls needed to create and join threads.  The
non-blocking version of the thread-per-client server (thread-per-client-nb)
uses two additional lines of code to place each socket into non-blocking
mode.  The remaining code logic needed to manage the non-blocking socket
calls is implemented in helper functions which are not included in the
analysis.

The thread-pool and event models are both much more complex than the
thread-per-client or {aesop} model.  In the case of the thread-pool server,
this additional complexity arises from not only the queuing and thread
management logic, but also the event loop which is necessary to detect
incoming requests and dispatch them to the queue.  The event server
complexity arises from the necessity of dividing servicing routines
into multiple sub-functions and manually tracking state between
those functions.  An additional complexity of the event
model which is not captured by these metrics is the fact that control flow
is not preserved across the processing of a given request.  For example,
servicing a write operation requires 5 disconnected event handlers.
Although the event model appears less complex than the thread-pool model according to CC and
Mod. CC, qualitatively it is significantly more challenging to develop.

[[sec-overhead-analysis]]
== Sources of {aesop} overhead

As we've seen {aesop} compares favorably to other concurrency models but loses
some performance compared to the best case hand tuned version. Here we examine
the overheads associated with {aesop}. The first item to examine is the
cost associated with an {aesop} blocking call.

=== Performance Implications of Blocking Calls

While this isn't immediately visible from looking at the {aesop} source code,
blocking calls, when compared to a plain C function call, have extra overhead
due to the way they are transformed by the {aesop} compiler. The following
section highlights the sources of this overhead.

==== State Management

Most of the overhead is caused by the need to preserve the state
of the blocking call while execution (temporarily) switches to another
function. In order to preserve this state, the {aesop} compiler relocates all
function-scoped variables from the stack to the heap.
When entering a blocking function, heap memory needs to be allocated for these
variables. As allocating heap memory is much more time consuming than
allocating space on the stack, calling a blocking function is more expensive
than calling a regular function.

==== Synchronization Overhead

A second source of overhead originates from the multi-threaded nature of {aesop}
code. While {aesop} does not create any threads, many of the {aesop} resources
internally use threads.  Therefore, the {aesop} compiler has to ensure that the
emitted code is multi-thread safe. For example, in the following code,
two pbranches might be executing concurrently using different threads.
Since these threads could both be modifying the pwait state simultaneously,
{aesop}
uses locks to serialize their access. Locks, and other synchronization
primitives account for most of the remaining performance difference between
blocking and regular function calls.

.pwait synchronization
[source,C]
----
pwait {
  pbranch {
     ...
  }
  pbranch {
     ...
  }
}
----

[NOTE]
Currently, the {aesop} compiler uses a combination of atomic operations and
mutexes to maintain thread-safety. There is an ongoing effort to convert to
atomic operations where possible.

==== Quantifying Blocking Call Overhead

For this test, a regular C function and a {aesop} blocking function are
called in a loop.  By timing the total time required to complete the loop,
an estimate of the time needed to execute the function call is obtained.

The results were obtained on an Intel i7 CPU running at 2.7 GHz,
using gcc 4.5.3 (using +-O2+), glibc 2.13-r4 and kernel 3.2.5.

There are a number of different test configurations:

.Test Results
[cols="h,^,^,^,^,^,<,<",valign="middle",frame="topbot",options="header"]
|=====
1.2+<.^| Test   5+| Options                              2+^.^| seconds/call  
          ^d| regular | blocking | malloc/free | mutex | opa   | malloc   | tcmalloc 
| 1        |   X     |          |             |       |       | 2.24e-09 | 2.21e-09
| 2        |         |    X     |             |       |       | 5.54e-08 | 3.62e-08
| 3        |   X     |          |     X       |       |       | 2.34e-08 | 1.62e-08
| 4        |   X     |          |     X       |   X   |       | 3.78e-08 | 2.96e-08
| 5        |   X     |          |     X       |       |   X   | 2.95e-08 | 2.02e-08
|=====

For test 1, a simple regular C function (i.e. not using `__blocking`) taking 2
arguments is used.  Test 2 uses the same function, but this time the
function is marked as `__blocking`.

Tests 3-5 were added to provide a better context for understanding the
magnitude of the blocking call overhead.  For test 3, the normal C function from test 1
was used as a basis, but a single call to +malloc+ and +free+ was
added within the test function.  Test 4 is the same as test 3, but also adds a call to lock and unlock
a mutex.  Test 5 replaces the mutex by a single atomic operation
(compare-and-swap).

As a way to study the effect of the malloc implementation, these tests were
also executed using a +tcmalloc+, an alternative memory allocator library.
The results for these are shown in the +tcmalloc+ column.

[NOTE]
The program used to obtain these results is in the repository:
+tests/blocking-overhead.ae+.

From these results we see that an {aesop} `__blocking` function is significantly
slower than a basic C function.  Test case 4 illustrates that almost all of
this overhead is a result of the memory management and synchronization
performed by {aesop} in order to enable efficient concurrency.

It is also important to note that {aesop} is a superset of the C language, and
normal C functions (with the associated low overhead) can still be used in
an {aesop} program.  The `__blocking` functions are most appropriate for
functions that perform blocking device or resource operations, while
standard C functions are better suited for computationally intense inner-loop routines.

=== Memory usage implications of blocking calls

The main memory overhead incurred by blocking functions originates
from the need to protect the logical state of the function while
temporarily switching to other functions.  Stack variables, function
arguments, and return values must all be moved to the heap by the
{aesop} source translator.  In a normal C program, those items consume
stack space. In addition, {aesop} internally maintains a number of control
structures. Pointers to these structures are passed as function arguments
to the blocking function, and consequently consume stack space. Currently,
{aesop} adds about 4 pointers and 2 integers to each blocking function call.

==== Lonely pbranches

A lonely pbranch will keep the enclosing scope alive (up to the function
scope) until the pbranch exits.

.lonely pbranch scope
[source, C]
----
__blocking int test (void)
{
   int var[10000];
   pbranch {
      ...
   }
}
----

So, in the example above, even though the test will return without waiting for
the pbranch to complete, its stack variables (`var` in this case) will
consume memory until the pbranch returns.

[[sec-compile]]
== Compile Time Performance

Currently {aesop} imposes some overhead when compiling {aesop} source. 
In a simple micro-benchmark an existing {aesop} source file was compiled and
then compiled again as straight C. The {aesop} source file contains 4043
lines and is about 116 KB in size. The same source file is renamed to a .c
file and four +#define+s are added which redefine the {aesop} keywords to nothing.

The test was executed on a dual processor Intel Xeon E5620 running at 2.4 GHz
with 24 GB of RAM.

.Compile Time Comparison
[[table-compile]]
[width="50%",cols="1,1>", options="header"]
|============================
| Type | Time (seconds)
| aesop | 112.02
| C | 1.22
|============================

The performance penalty is significant but only effects development. It
can be mitigated by constructing a Makefile that supports parallel make.

== Bibliography

[bibliography]
- [[[McCabe]]] McCabe, T.J. A Complexity Measure. In IEEE Transactions on
  Software Engineering, vol.SE-2, no.4, pp. 308- 320, Dec. 1976.
- [[[Bame]]] Paul Bame.  pmccabe.  http://parisc-linux.org/~bame/pmccabe/
- [[[Wheeler]]] David A. Wheeler.  sloccount.  http://www.dwheeler.com/sloc/
- [[[libev]]] Marc Lehmann. http://software.schmorp.de/pkg/libev.html
- [[[hp-cond-variable]]] Hewlet Packard Development Company L.P..  "Techniques for
  Improving the Scalability of Applications Using POSIX Thread
  Condition Variables."
  http://h21007.www2.hp.com/portal/download/files/unprot/hpux/MakingConditionVariablesPerform.pdf

== Appendix A - Raw Data

.Read Runtime Data
[[table-readhist]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  0.794610 |  0.706736 |  0.775575 |  2.135912 |  0.861456 |  1.284919 
|  128 |  6.740836 |  3.755610 |  3.865248 |  3.395985 |  5.688868 |  9.602547 
|  256 | 12.352416 | 12.936459 | 13.051936 | 12.383918 | 11.434637 | 19.542018 
|  512 | 23.641337 | 27.395763 | 26.318174 | 24.776392 | 22.944439 | 39.058706 
| 1024 | 48.309275 | 54.852327 | 55.658511 | 47.564368 | 46.179671 | 77.216961 
|============================

.Write Runtime Data
[[table-writehist]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
|clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |   3.200944 |   2.306964 |   2.157996 |   1.934971 |   2.684861 |   2.512977 
|  128 |  18.213375 |  14.991499 |  15.802083 |  16.594684 |  18.065047 |  17.075809 
|  256 |  33.515213 |  32.899976 |  32.548540 |  32.662463 |  35.354696 |  33.900419 
|  512 |  67.640246 |  66.168558 |  66.324414 |  67.188763 |  71.184339 |  69.184007 
| 1024 | 119.799388 | 130.895169 | 130.829158 | 140.270790 | 141.208043 | 140.464092 
|============================


.Read-Null Runtime Data
[[table-readnull]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1.122259 |  1.688565 |  1.845739 |  1.511863 |  1.220004 |  1.225304 
|  128 |  3.640566 |  4.558670 |  5.159914 |  9.884832 |  4.002515 |  7.252784 
|  256 |  7.087344 |  5.400444 |  6.604424 | 19.875843 |  7.448785 | 14.500385 
|  512 | 13.661447 | 10.603367 | 12.862459 | 40.884105 | 14.549549 | 28.967834 
| 1024 | 27.013576 | 21.625077 | 25.814349 | 82.675118 | 29.376645 | 50.822263 
|============================

.Write-Null Runtime Data
[[table-writenull]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1.007547 |  0.932891 |  1.012248 |  1.561492 |   1.368119 |  1.372057 
|  128 |  4.844869 |  3.680892 |  4.771543 | 10.240339 |  16.074748 |  7.923767 
|  256 |  9.263294 |  6.997207 |  9.135328 | 20.342675 |  29.219365 | 15.994714 
|  512 | 18.293856 | 14.069904 | 18.382822 | 41.136291 |  59.712499 | 32.640966 
| 1024 | 37.081757 | 28.534744 | 39.618136 | 83.716303 | 116.833184 | 58.592679 
|============================

.Read Time Data
[[table-readtime]]
[cols="1>,1>,1>,1>", options="header"]
|============================
| server | min | max | x
| aesop | 0.041914 | 48.309275 | 1.0
| thread-per-client | 0.055891 | 54.852327 | 2.0
| thread-per-client-nb | 0.067599 | 55.658511 | 3.0
| thread-per-op | 0.667692 | 47.564368 | 4.0
| thread-pool | 0.073988 | 46.179671 | 5.0
| event | 63.155711 | 77.216961 | 6.0
|============================

.Write Time Data
[[table-writetime]]
[cols="1>,1>,1>,1>", options="header"]
|============================
| server | min | max | x
| aesop | 0.415026 | 119.799388 | 1.0
| thread-per-client | 67.926019 | 130.895169 | 2.0
| thread-per-client-nb | 77.042924 | 130.829158 | 3.0
| thread-per-op | 77.181582 | 140.270790 | 4.0
| thread-pool | 0.915993 | 141.208043 | 5.0
| event | 119.316831 | 140.464092 | 6.0
|============================

.Read-Null Time Data
[[table-readnulltime]]
[cols="1>,1>,1>,1>", options="header"]
|============================
| server | min | max | x
| aesop | 16.165011 | 27.013576 | 1.0
| thread-per-client | 9.041217 | 21.625077 | 2.0
| thread-per-client-nb | 15.326102 | 25.814349 | 3.0
| thread-per-op | 63.169438 | 82.675118 | 4.0
| thread-pool | 20.796985 | 29.376645 | 5.0
| event | 35.460415 | 50.154710 | 6.0
|============================

.Write-Null Time Data
[[table-writenulltime]]
[cols="1>,1>,1>,1>", options="header"]
|============================
| server | min | max | x
| aesop | 26.075373 | 37.081757 | 1.0
| thread-per-client | 20.131762 | 28.534744 | 2.0
| thread-per-client-nb | 29.589322 | 39.618136 | 3.0
| thread-per-op | 69.875107 | 83.716303 | 4.0
| thread-pool | 11.849568 | 116.833184 | 5.0
| event | 51.039087 | 58.592679 | 6.0
|============================

.Read Latency Data
[[table-readlat]]
[cols="1>,1>,1>,1>,1>,1>", options="header"]
|============================
| server | first | min | max | third | x
| aesop | 0.000257 | 0.000251 | 0.000268 | 0.000266 | 1.0
| thread-per-client | 0.000250 | 0.000242 | 0.000267 | 0.000265 | 2.0
| thread-per-client-nb | 0.000249 | 0.000242 | 0.000265 | 0.000264 | 3.0
| thread-per-op | 0.000462 | 0.000340 | 0.000495 | 0.000489 | 4.0
| thread-pool | 0.000256 | 0.000239 | 0.000278 | 0.000275 | 5.0
| event | 0.016989 | 0.006868 | 0.034917 | 0.028945 | 6.0
|============================

.Write Latency Data
[[table-writelat]]
[cols="1>,1>,1>,1>,1>,1>", options="header"]
|============================
| server | first | min | max | third | x
| aesop | 0.003235 | 0.002687 | 0.004293 | 0.003931 | 1.0
| thread-per-client | 0.005229 | 0.002843 | 0.008247 | 0.008217 | 2.0
| thread-per-client-nb | 0.004103 | 0.001798 | 0.008219 | 0.008124 | 3.0
| thread-per-op | 0.017350 | 0.011734 | 0.037548 | 0.030839 | 4.0
| thread-pool | 0.003174 | 0.002525 | 0.004169 | 0.003877 | 5.0
| event | 0.045139 | 0.011745 | 0.103348 | 0.103275 | 6.0
|============================

.Read-Null Latency Data
[[table-readnulllat]]
[cols="1>,1>,1>,1>,1>,1>", options="header"]
|============================
| server | first | min | max | third | x
| aesop | 0.000066 | 0.000041 | 0.000083 | 0.000077 | 1.0
| thread-per-client | 0.000062 | 0.000041 | 0.000074 | 0.000070 | 2.0
| thread-per-client-nb | 0.000067 | 0.000044 | 0.000086 | 0.000082 | 3.0
| thread-per-op | 0.000117 | 0.000062 | 0.000331 | 0.000257 | 4.0
| thread-pool | 0.000078 | 0.000048 | 0.000102 | 0.000096 | 5.0
| event | 0.000077 | 0.000044 | 0.000135 | 0.000116 | 6.0
|============================

.Write-Null Latency Data
[[table-writenulllat]]
[cols="1>,1>,1>,1>,1>,1>", options="header"]
|============================
| server | first | min | max | third | x
| aesop | 0.000070 | 0.000040 | 0.000101 | 0.000091 | 1.0
| thread-per-client | 0.000065 | 0.000042 | 0.000079 | 0.000076 | 2.0
| thread-per-client-nb | 0.000064 | 0.000042 | 0.000081 | 0.000077 | 3.0
| thread-per-op | 0.000110 | 0.000062 | 0.000379 | 0.000302 | 4.0
| thread-pool | 0.000049 | 0.000036 | 0.000053 | 0.000052 | 5.0
| event | 0.000074 | 0.000046 | 0.000184 | 0.000133 | 6.0
|============================

.Read Memory Data
[[table-readmem]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1720 |   840 |   864 |   928 |  860 |   984 
|  128 |  2720 |  2748 |  2600 |  2832 | 1040 |  2988 
|  256 |  4700 |  5256 |  5064 |  5076 | 1160 |  5520 
|  512 |  5284 |  9316 |  9564 |  9352 | 1344 |  8428 
| 1024 | 10276 | 18216 | 18668 | 18288 | 1656 | 19248 
|============================

.Write Memory Data
[[table-writemem]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1692 |   844 |   864 |   944 |  876 |   956 
|  128 |  3152 |  2912 |  2852 |  2932 | 1044 |  2080 
|  256 |  3912 |  5436 |  5152 |  5104 | 1192 |  3292 
|  512 |  6764 |  9844 |  9960 |  9760 | 1316 |  5604 
| 1024 | 14700 | 18648 | 18940 | 18920 | 1652 | 19284 
|============================

.Read-Null Memory Data
[[table-readnullmem]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 | 1580 |  836 |  844 |  864 |  920 |  640 
|  128 | 2060 | 1972 | 1788 | 1000 | 1104 |  696 
|  256 | 2244 | 2992 | 2868 | 1000 | 1212 |  768 
|  512 | 2868 | 5200 | 5004 | 1124 | 1388 |  904 
| 1024 | 3900 | 9752 | 9292 | 4948 | 1744 | 1128 
|============================

.Write-Null Memory Data
[[table-writenullmem]]
[cols="1>,1>,1>,1>,1>,1>,1>", options="header"]
|============================
| clients | aesop | thread | thread-nb | thread-per-op | thread-pool | event
|   16 |  1600 |   856 |   816 |   852 |  916 |  680 
|  128 |  2876 |  2632 |  2652 |  1728 | 1144 |  744 
|  256 |  4588 |  5176 |  4904 |  1036 | 1248 |  808 
|  512 |  8036 |  9728 |  9604 |  7184 | 1436 | 1336 
| 1024 | 14084 | 19060 | 18764 | 11068 | 1808 | 5632 
|============================
