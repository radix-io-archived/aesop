= Aesop Performance Analysis

== Introduction

We examine the performance of Aesop based on several factors including
runtime performance, memory effieceny, programmer productivity and 
compile time performance. We evaluated aesop by building a simple TCP server
which would recieve requests from clients and then either recieve or send
data which was either generated, discarded, read from a file or written to
a file. This server design was then implemented in aesop and then in generic
C code using other concurrency models for comparison. The description of
the test server and client are below and these serve as the basis for the
evaluations in the rest of the text. We also explore some micro-benchmarks
to analyze how core features of aesop impact performance.

=== Client Design

The client is a basic C program that uses TCP sockets to send messages to
the server. The client will run in a loop generating a specifed number of
operations to the server. The general flow is that client send a request
to the server that contains and optional payload. The client then waits for
the server to send an acknowledgement that also contain an optional payload.

[float]
==== Request Types

The client supports the following request types.

[float]
===== Read

The client sends a request with a file name and a size. The server will then
open the file, read the contents up to the size specified. The server returns
the data with the acknowledgement of the operation.

[float]
===== Write

The client sends a request with a file name, size and payload. The server will
then create the file and write the payload. The server then sends an
acknowledgement to client.

[float]
===== Read-Null

The client sends a request with a size to the server. The server then allocates
a buffer for the response based on the size the client requested. The server
then sends an acknowledgement with this buffer as the payload.

[float]
===== Write-Null

The client sends a request with a size and a payload. The server recieves 
the request but then simply discards the payload. The server sends an 
acknowledgement back to the client.

[float]
==== Implementation

The client has a set of command line parameters which control selecting the
test type, the number of requets and the size of the request. The client is
also a MPI program. This allows starting an arbitray number of clients and
then synchronizing startup. The clients all barrier until they are ready to
connect to the server. Clients exit the barrier, connect to the server
and begin sending requests in a loop. When the client completes it waits
at another barrier and then reports statistics about the run.

=== Server Design

The server supports the client request types listed above. Each server
implementation accepts client connections and then waits for requests.

.Server Request Handling
[graphviz]
--------
digraph G
{
   subgraph I {
  rank = same;
   incoming [label="client connects", shape="box"];
   close [label="close connection"];
   }

   wait [label="receive request"];

   incoming -> wait;
   wait -> close [label="client closes connection"];
   wait -> read_1 [label = "READ"];
   wait -> readn_1 [label = "READ NULL"];
   wait -> write_1 [label = "WRITE"];
   wait -> writen_1 [label = "WRITE NULL"];

   subgraph R {
   read_1 [label="read from file"];
   read_2 [label="send data"];
   read_1 -> read_2 -> wait;
   }

   subgraph W {
   write_1 [label="receive data"];
   write_2 [label="write to file"];
   write_1 -> write_2 -> wait;
   }

   subgraph RN {
   readn_1 [label="send data"];
   readn_1 -> wait;
   }
   
   subgraph WN {
   writen_1 [label="receive data"];
   writen_1 -> wait;
   }
}
--------

The servers share a common base of code for handling the requests.
This insures that the variation in performance is primarily due
to concurrency within the server. There is one exception to this which is
the use of blocking or non-blocking sockets. This will be ellaborated on
more in the analysis. All event loops are implemented using libev. <<libev>>
The thread implementations all use `pthread` threading library.

==== Server Implementations

We implemented six server types including Aesop. Each server type examines
a different type of concurrency model.
 
[float]
===== Aesop

The Aesop server is done using Aesop of course. The server uses a
'lonely pbranch' to service each client. All operations for a client are
handled within a single pbranch. The underlying socket resource uses
non-blocking sockets with a thread pool of 16 threads. The file resource uses
synchronous IO and a thread pool with 4 threads.

[float]
===== Thread-per-client

The thread-per-client server spawns a thread for each client connection. All
requests for the client are handled within the same thread. This model uses
blocking sockets. The thread remains alive until the client disconnects. 

[float]
===== Thread-per-client-nb

The thread-per-client-nb server is identical to the thread-per-client server
except that is uses non-blocking sockets instead of blocking sockets. We
implemented this version to investigate the possible performance difference
between the synchronous and asynchronous socket calls.

[float]
===== Thread-per-operation
The thread-per-operation server uses and event loop to watch all sockets for
activity and when requests are available a thread is spawned and the request
is handled completely from within that thread. When the request is complete
the thread is destroyed. Blocking sockets are used in this implementation.

[float]
===== Thead-pool
The thread-pool server uses and event loop to watch all sockets for activity.
When requests are available, the event loop puts the request on a queue and
wakes up a thread from the thread pool. The request is handled completely
from within a single thread of the thread pool. Blocking sockets are used
in this implementation.

[float]
===== Event
The event server uses a execution context to handle all clients and requests.
The event loop watches all sockets and handles each request in a callback. The
event server uses non-blocking sockets and asynchronous file I/O. Note that
the though the operating system can still use multiple cores to drive the
network and disk.

== Runtime Performance Evaluation and Analysis

The evaluation of runtime performance was done by executing a series of tests
using each server implementation type with the same client as discussed above.
We then compare the results for each of the server implementations against
aesop determine the overall effiecency of aesop compared to hand-tuned 
solutions. The four test types were selected to evaluate two models. The
first two test types (read,write) examine real disk I/O. Aesop is inteneded
for use in a storage server so we want to examine multiplexing of disk and
network I/O. The second set (read-null,write-null) eliminates disk I/O and
only has network I/O. This removes the storage bottleneck and examines how
well the different server types handle concurrency.

=== Experiment

We ran our tests on the Argonne Fusion cluster which is a standard Linux HPC
cluster. The experiment was done by running each client mode (read, write,
read-null, write-null) against all six server types. 

The testing was done by running a job on Fusion that executed one of the client
test types against all six server implementations scaling the clients from
16 to 1024. This kept the wall time of the job reasonable and allows a 
consistent comparison between server types within a test type. The network
is still a shared resource on the cluster or each individual test was run
five times and the results below are the median from the five iterations,
based on the maximum runtime.

==== Experiment System

Fusion is a cluster run by the Argonne Laboratory Computing Resource Center
(LCRF). Fusion is a IBM iDataPlex dx360 M2 system. It features 320 compute
nodes which consist of two Intel Nehalem 2.6 GH Xeon processors and 36 GB
of RAM. The compute nodes have hyper threading disabled. The cluster has
an Infiniband QDR interconnect. Each compute node also a single SATA 7200 RPM
hard disk for local scratch storage.

==== Experiment Details

The experiment consisted of four separate jobs on Fusion. The top level unit
is the test type: read, write, read-null, write-null. For each test type,
the number of clients are scaled up from 16 to 1024. At each client scale,
each server implementation is tested in sequence. The individual test is
executed five times. On each iteration, the server is started then the MPI
client program is started. The server is restarted for each iteration so
that the memory statistics are not polluted by a previous run.

On fusion we determined we could use 16 clients per physical node. Using
more clients per node caused the bottleneck of the test to become the client
nodes instead of the server. The clients were scaled up as follows: 16, 128, 256, 512 and 1024 clients.

The clients connected to the server using the IB network with IPoIB.

[float]
===== Read

The read test had clients each issue 16 requests asking for 4 KiB from
disk. Each client specifies a unique file to be read on on each request. All
clients specifiy unique files. The files are first generated by a script
that runs before the read test starts. The script generates files for every
client in the local storage of the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 16 --size 4096 --type-r
*********************

[float]
===== Write

The write test had clients each issue 16 requests sending 4 KiB of data to
be written to disk. Each client specifies a unique file name for each request
and all clients specifiy unique files from each other. The directory
containing all the files is deleted between each test iteration.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 16 --size 4096 --type-w
*********************

[float]
===== Read-Null

The read-null test had clients each issue 4096 requests requesting 4 KiB of
data to be returned from the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 4096 --size 4096 --type-rn
*********************

[float]
===== Write-Null

The write-null test had clients each issue 4096 requests sending 4 KiB of 
data to be discarded by the server.

.Execution Parameters
*********************
mpirun -np <procs> echo-client --ip <ip> --port 9999 --path <path> --num-requests 4096 --size 4096 --type-wn
*********************

=== Evaluation

Here the runtime results are presented from the experimentation. All graphs
are shown in log scale.

==== Disk I/O

In <<fig-readhist>> and <<fig-writehist>>, we see that aesop fairs
favorably to the other server implementations. In general the various
servers show similar scaling.

.Runtime Performance for Read Test
[[fig-readhist]]
image::fig/read-hist.png[]

.Runtime Performance for Write Test
[[fig-writehist]]
image::fig/write-hist.png[]

==== Network I/O

In this set of tests, we had specifically developed the thread-per-client-nb 
to evalute the performance difference between blocking and non-blocking sockets.It is clear that the non-blocking sockets do not perform as well blocking
sockets. We did this test because aesop uses non-blocking sockets and were
trying to determine the performance difference between aesop and
the thread-per-client server. <<fig-readnull>> and <<fig-writenull>> cases show
that aesop mirrors the thread-per-client-nb server as expected.

.Runtime Performance for Read-Null Test
[[fig-readnull]]
image::fig/read-null-hist.png[]

.Runtime Performance for Write-Null Test
[[fig-writenull]]
image::fig/write-null-hist.png[]

==== Fairness

Absolute performance is not the only metric that one might look at so here
the fairness of the various server types is examined. The bars on the 
graphs shows the total time for a client to complete all of its requests.
The fastest client is the bottom of the bar and the slowest is the top of bar.

//.Fastest and Slowest Total Client Runtime for Read Test
//[[fig-readtime]]
//image::fig/read-time.png[]

//.Fastest and Slowest Total Client Runtime for Write Test
//[[fig-writetime]]
//image::fig/write-time.png[]

=== Runtime Analysis

Aesop compares favorably to other concurrency models but will of course lose
some performance compared to best case hand tuned version. Here we examine
the overheads associated with aesop. The first item to examine is the
cost associated with an aesop blocking call.

==== Performance Implications of Blocking Calls

While this isn't immediately visible from looking at the aesop source code,
blocking calls, when compared to a plain C function call, have extra overhead
due to the way they are transformed by the aesop compiler. The following
section highlights the sources of this overhead.

===== Understanding Blocking Call Overhead

[float]
===== State Management

Most of the overhead is caused by the need to preserve the state
of the blocking call while execution (temporarily) switches to another
function. In order to preserve this state, the aesop compiler relocates all
function-scoped variables from the stack to the heap.
When entering a blocking function, heap memory needs to be allocated for these
variables. As allocating heap memory is much more time consuming than
allocating space on the stack, calling a blocking function is more expensive
than calling a regular function.

[float]
===== Synchronization Overhead

A second source of overhead originates from the multi-threaded nature of aesop
code. While aesop does not create any threads, many of the aesop resources
internally use threads.  Therefore, the aesop compiler has to ensure that the
emitted code is multi-thread safe. For example, in the following code,
two pbranches might be executing concurrently using different threads.
Since these threads could both by modifying the pwait state simultaneously,
aesop
uses locks to serialize their access. Locks, and other synchronization
primitives account for most of the remaining performance difference between
blocking and regular function calls.

.pwait synchronization
[source,C]
----
pwait {
  pbranch {
     ...
  }
  pbranch {
     ...
  }
}
----

[NOTE]
Currently, the aesop compiler uses a combination of atomic operations and
mutexes to maintain thread-safety. There is an ongoing effort to convert to
atomic operations where possible.

[float]
===== Quantifying Blocking Call Overhead

For this test, a regular and a blocking function are called in a loop.
By timing the total time required to complete the loop, an estimate of the
time needed to execute the function call is obtained.

The results were obtained on an intel i7 CPU running at 2.7GHz,
using gcc 4.5.3 (using +-O2+), glibc 2.13-r4 and kernel 3.2.5.

There are a number of different test configurations:

.Test Results
[width="20%",cols="h,^,^,^,^,^,<,<",valign="middle",frame="topbot",options="header"]
|=====
1.2+<.^| Test   5+| Options                              2+^.^| seconds/call  
          ^d| regular | blocking | malloc/free | mutex | opa   | malloc   | tcmalloc 
| 1        |   X     |          |             |       |       | 2.24e-09 | 2.21e-09
| 2        |         |    X     |             |       |       | 5.54e-08 | 3.62e-08
| 3        |   X     |          |     X       |       |       | 2.34e-08 | 1.62e-08
| 4        |   X     |          |     X       |   X   |       | 3.78e-08 | 2.96e-08
| 5        |   X     |          |     X       |       |   X   | 2.95e-08 | 2.02e-08
|=====

For test 1, a simple regular C function (i.e. not using `__blocking`) taking 2
arguments is used.
Test 2 uses the same function, but this time the function is marked as
`__blocking`.

Tests 3-5 were added to provide a better context for understanding the
magnitude of the blocking call overhead.  For test 3, the function from test 1
was taken but in the function body a single call to +malloc+ and +free+ was
added.  Test 4 is the same as test 3, but also adds a call to lock and unlock
a mutex.  Test 5 replaces the mutex by a single atomic operation
(compare-and-swap).

As a way to study the effect of the malloc implementation, these tests were
also executed using a +tcmalloc+, an alternative memory allocator library.
The results for these are shown in the tcmalloc column.

[TIP]
The progam used to obtain these results is in the repository:
+tests/blocking-overhead.ae+.


=== Code Complexity

As a measure for productivity, we investigated the code of each server using a
set of complexity metrics.

.Implementation complexity analysis.
[[table-complex]]
[cols="3,1,1,1", options="header"]
|============================
| Server Implementation | CC | Mod. CC | SLOC 
| aesop | 16 | 11 | 179 
| thread-per-client | 17 | 12 | 182
| thread-per-client-nb | 17 | 12 | 184
| thread-per-op | 22 | 17 | 249
| thread-pool | 32 | 26 | 313
| event | 28 | 23 | 341
|============================

//////
\begin{table}
\small
\begin{center}
\caption{Complexity analysis for example servers}
\begin{tabular}{lrrr}
\hline
& CC & mod. CC & SLOC \\
\hline
\hline
\end{tabular}
\label{tab:complexity}
\end{center}
\normalsize
\vspace{-.2in}
\end{table}
/////

<<table-complex>> compares the code complexity of each server
implementation using McCabe Cyclomatic Complexity (CC) <<McCabe>>,
Modified McCabe Cyclomatic Complexity (Mod. CC), and Source Lines of Code
(SLOC).  The CC and Mod. CC metrics were measured using the pmccabe tool,
version 2.6, created by Paul Bame <<Bame>>, while the SLOC metrics were
measured using the sloccount tool, version 2.26,
created by David A. Wheeler <<Wheeler>>.

To simplify the comparison, error handling was excluded for all servers
except for assertions on expected return codes.  The protocol definition
(ie, request and acknowledgement structs) as well as helper functions to
loop over send and receive were not counted in any of the implementations,
as these were similar in all four.  Also note that the Aesop version
does not include the Aesop standard library, which 
provides a binding between Aesop and the standard POSIX socket API as part
of its default functionality.  The focus
of this comparison is on the core logic defining the server implementation.

The Aesop and thread-per-client servers are very similar in terms of complexity.  The
slight increase in complexity for the thread-per-client server results from
the additional function calls needed to create and join threads.  The
nonblocking version of the thread-per-client server (thread-per-client-nb)
uses two additional lines of code to place each socket into non-blocking
mode.  The remaining code logic needed to manage the nonblocking socket
calls is implemented in helper functions which are not included in the
analysis.

The thread-pool and event models are both much more complex than the
thread-per-client or aesop model.  In the case of the thread-pool server,
this additional complexity arises from not only the queueing and thread
management logic, but also the event loop which is necessary to detect
incoming requests and dispatch them to the queue.  The event server
complexity arises from the necessity of dividing servicing routines into multiple
sub-functions and manually tracking state between those functions.
An additional complexity of the event
model which is not captured by these metrics is the fact that control flow
is not preserved across the processing of a given request.  For example,
servicing a write operation requires 5 disconnected event handlers.
Although the event model appears less complex than the thread-pool model according to CC and
Mod. CC, qualitatively it is significantly more challenging to develop.

=== Performance Evaluation

We evaluated the performance of our server implementation.
The same client was used for all server implementations.

.Performance writing to server memory.
[[fig-write]]
image::fig/write-hist.png["Write"]

.Performance reading from server memory.
[[fig-read]]
image::fig/read-hist.png["Read"]

.Performance writing to disk.
[[fig-write-null]]
image::fig/write-null-hist.png["Writing to disk"]

.Performance reading from disk.
[[fig-read-null]]
image::fig/read-null-hist.png["Reading from disk"]

.Memory usage during the write test
[[fig-write-mem]]
image::fig/write-mem.png["Write test server memory usage"]

.Memory usage during the read test
[[fig-read-mem]]
image::fig/read-mem.png["Read test server memory usage"]

.Memory usage during the write-null test
[[fig-write-mem]]
image::fig/write-null-mem.png["Write-null test server memory usage"]

.Memory usage during the read-null test
[[fig-read-mem]]
image::fig/read-null-mem.png["Read-null test server memory usage"]

== Runtime Memory Efficiency

This section examines the memory efficiency of the translated aesop code.

=== Function arguments and stack variables of blocking calls

Aesop, in order to implement the additional functionality provided by blocking
calls, rewrites blocking calls when translating the aesop code to C code.
This translation introduces a certain amount of overhead, both in memory usage
and execution performance. This section focuses on memory overhead,
deferring the discussion of execution overhead to <<ref-blocking-runtime>>.

The main memory overhead incurred by blocking functions originates from the
need to protect the logical state of the function while temporarily switching
to other functions.

For example, stack variables are moved to the heap. As long as the blocking
function does not complete, the memory for these variables is not released.
Arguments to the function need to be relocated to the heap as well,
and so does the type returned from the function (if not void).

In a normal C program, the items listed above consume stack space. In blocking
functions, these consume heap space instead.  In addition, aesop internally
maintains a number of control structures. Pointers to these structures are
passed as function arguments to the blocking function, and consequently
consume stack space. Currently, aesop adds about 4 pointers and 2 integers to
each blocking function call.

=== Lonely pbranches

A lonely pbranch will keep the enclosing scope alive (up to the function
scope) until the pbranch exits.

.lonely pbranch scope
[source, C]
----
__blocking int test (void)
{
   int var[10000];
   pbranch {
      ...
   }
}
----

So, in the example above, even though the test will return without waiting for
the pbranch to complete, it's stack variables (`var` in this case) will
consume memory until the pbranch returns.




== Compile Time Performance

=== Parallel Make

It is possible to speed up the translation of aesop files by using the `-jn`
option to make, replacing `n` by the desired number of concurrent jobs.

=== CCache

At this time, there are a number of issues blocking the use  of ccache in
combination with the aesop source to source translator (either to cache the
translation or to cache the compilation of the generated C source code).

A first issue is related to incorrect handling of compiler names in the build
system, causing the build to fail if the compiler is set to `ccache gcc`.

The second issue stems from the fact that aesop introduces additional
dependencies which are not understood by ccache. Therefore, subtly failures
would be introduced when aesop is updated and the cache is not manually
cleared.

Given these issues, at this point it is not recommended to use ccache in
combination with aesop. However, both issues can be resolved in a later
aesop release (see ticket #137 and #200 in the triton repository).




== Bibliography

[bibliography]
- [[[McCabe]]] McCabe, T.J. A Complexity Measure. In IEEE Transactions on
  Software Engineering, vol.SE-2, no.4, pp. 308- 320, Dec. 1976.
- [[[Bame]]] Paul Bame.  pmccabe.  http://parisc-linux.org/~bame/pmccabe/
- [[[Wheeler]]] David A. Wheeler.  sloccount.  http://www.dwheeler.com/sloc/
- [[[libev]]] Marc Lehmann. http://software.schmorp.de/pkg/libev.html
