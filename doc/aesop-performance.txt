= Aesop Performance Evaluation and Analysis

== Introduction

We examine the performance of Aesop based on several factors including
runtime performance, memory effieceny, programmer productivity and 
compile time performance. We evaluated aesop by building a simple TCP server
which would recieve requests from clients and then either recieve or send
data which was either generated, discarded, read from a file or written to
a file. This server design was then implemented in aesop and then in generic
C code using other concurrency models for comparison. The description of
the test server and client are below and these serve as the basis for the
evaluations in the rest of the text. We also explore some micro-benchmarks
to analyze how core features of aesop impact performance.

=== Client Design

The client is a basic C program that uses TCP sockets to send messages to
the server. The client will run in a loop generating a specifed number of
operations to the server. The general flow is that client send a request
to the server that contains and optional payload. The client then waits for
the server to send an acknowledgement that also contain an optional payload.

[float]
==== Request Types

The client supports the following request types.

[float]
===== Read

The client sends a request with a file name and a size. The server will then
open the file, read the contents up to the size specified. The server returns
the data with the acknowledgement of the operation.

[float]
===== Write

The client sends a request with a file name, size and payload. The server will
then create the file and write the payload. The server then sends an
acknowledgement to client.

[float]
===== Read-Null

The client sends a request with a size to the server. The server then allocates
a buffer for the response based on the size the client requested. The server
then sends an acknowledgement with this buffer as the payload.

[float]
===== Write-Null

The client sends a request with a size and a payload. The server recieves 
the request but then simply discards the payload. The server sends an 
acknowledgement back to the client.

[float]
==== Implementation

The client has a set of command line parameters which control selecting the
test type, the number of requets and the size of the request. The client is
also a MPI program. This allows starting an arbitray number of clients and
then synchronizing startup. The clients all barrier until they are ready to
connect to the server. Clients exit the barrier, connect to the server
and begin sending requests in a loop. When the client completes it waits
at another barrier and then reports statistics about the run.

=== Server Design

The server supports the client request types listed above. Each server
implementation accepts client connections and then waits for requests.

.Server Request Handling
[graphviz]
--------
digraph G
{
   subgraph I {
  rank = same;
   incoming [label="client connects", shape="box"];
   close [label="close connection"];
   }

   wait [label="receive request"];

   incoming -> wait;
   wait -> close [label="client closes connection"];
   wait -> read_1 [label = "READ"];
   wait -> readn_1 [label = "READ NULL"];
   wait -> write_1 [label = "WRITE"];
   wait -> writen_1 [label = "WRITE NULL"];

   subgraph R {
   read_1 [label="read from file"];
   read_2 [label="send data"];
   read_1 -> read_2 -> wait;
   }

   subgraph W {
   write_1 [label="receive data"];
   write_2 [label="write to file"];
   write_1 -> write_2 -> wait;
   }

   subgraph RN {
   readn_1 [label="send data"];
   readn_1 -> wait;
   }
   
   subgraph WN {
   writen_1 [label="receive data"];
   writen_1 -> wait;
   }
}
--------

The servers share a common base of code for handling the requests.
This insures that the variation in performance is primarily due
to concurrency within the server. There is one exception to this which is
the use of blocking or non-blocking sockets. This will be ellaborated on
more in the analysis. All event loops are implemented using libev. <<libev>>

==== Server Implementations

We implemented six server types including Aesop. Each server type examines
a different type of concurrency model.
 
[float]
===== Aesop

The Aesop server is done using Aesop of course. The server uses a
'lonely pbranch' to service each client. All operations for a client are
handled within a single pbranch. The underlying socket resource uses
non-blocking sockets with a thread pool of 16 threads. The file resource uses
synchronous IO and a thread pool with 4 threads.

[float]
===== Thread-per-client

The thread-per-client server spawns a thread for each client connection. All
requests for the client are handled within the same thread. This model uses
blocking sockets.

[float]
===== Thread-per-client-nb

The thread-per-client-nb server is identical to the thread-per-client server
except that is uses non-blocking sockets instead of blocking sockets.

[float]
===== Thread-per-operation
The thread-per-operation server uses and event loop to watch all sockets for
activity and when requests are available a thread is spawned and the request
is handled completely from within that thread. Blocking sockets are used
in this implementation.

[float]
===== Thead-pool
The thread-pool server uses and event loop to watch all sockets for activity.
When requests are available, the event loop puts the request on a queue and
wakes up a thread from the thread pool. The request is handled completely
from within a single thread of the thread pool. Blocking sockets are used
in this implementation.

[float]
===== Event
The event server uses a single thread to handle all clients and requests. The
event loop watches all sockets and handles each request in a callback. The
event server uses non-blocking sockets and asynchronous file I/O.

== Runtime Performance Evaluation and Analysis

The evaluation of runtime performance was done by executing a series of tests
using each server implementation type with the same client as discussed above.
We then compare the results for each of the server implementations against
aesop determine the overall effiecency of aesop compared to hand-tuned 
solutions.

=== Experiment

We ran our tests on the Argonne Fusion cluster which is a standard Linux HPC
cluster. The experiment was done 

[[ref-blocking-runtime]]
=== Performance Implications of Blocking Calls

While this isn't immediately visible from looking at the aesop source code,
blocking calls, when compared to a plain C function call, have extra overhead
due to the way they are transformed by the aesop compiler. The following
section highlights the sources of this overhead.

==== Understanding Blocking Call Overhead

===== State Management

Most of the overhead is caused by the need to preserve the state
of the blocking call while execution (temporarily) switches to another
function. In order to preserve this state, the aesop compiler relocates all
function-scoped variables from the stack to the heap.
When entering a blocking function, heap memory needs to be allocated for these
variables. As allocating heap memory is much more time consuming than
allocating space on the stack, calling a blocking function is more expensive
than calling a regular function.



===== Synchronization Overhead

A second source of overhead originates from the multi-threaded nature of aesop
code. While aesop does not create any threads, many of the aesop resources
internally use threads.  Therefore, the aesop compiler has to ensure that the
emitted code is multi-thread safe. For example, in the following code,
two pbranches might be executing concurrently using different threads.
Since these threads could both by modifying the pwait state simultaneously,
aesop
uses locks to serialize their access. Locks, and other synchronization
primitives account for most of the remaining performance difference between
blocking and regular function calls.

.pwait synchronization
[source,C]
----
pwait {
  pbranch {
     ...
  }
  pbranch {
     ...
  }
}
----


[NOTE]
Currently, the aesop compiler uses a combination of atomic operations and
mutexes to maintain thread-safety. There is an ongoing effort to convert to
atomic operations where possible.


==== Quantifying Blocking Call Overhead


For this test, a regular and a blocking function are called in a loop.
By timing the total time required to complete the loop, an estimate of the
time needed to execute the function call is obtained.

The results were obtained on an intel i7 CPU running at 2.7GHz,
using gcc 4.5.3 (using +-O2+), glibc 2.13-r4 and kernel 3.2.5.

There are a number of different test configurations:

.Test Results
[width="20%",cols="h,^,^,^,^,^,<,<",valign="middle",frame="topbot",options="header"]
|=====
1.2+<.^| Test   5+| Options                              2+^.^| seconds/call  
          ^d| regular | blocking | malloc/free | mutex | opa   | malloc   | tcmalloc 
| 1        |   X     |          |             |       |       | 2.24e-09 | 2.21e-09
| 2        |         |    X     |             |       |       | 5.54e-08 | 3.62e-08
| 3        |   X     |          |     X       |       |       | 2.34e-08 | 1.62e-08
| 4        |   X     |          |     X       |   X   |       | 3.78e-08 | 2.96e-08
| 5        |   X     |          |     X       |       |   X   | 2.95e-08 | 2.02e-08
|=====

For test 1, a simple regular C function (i.e. not using `__blocking`) taking 2
arguments is used.
Test 2 uses the same function, but this time the function is marked as
`__blocking`.

Tests 3-5 were added to provide a better context for understanding the
magnitude of the blocking call overhead.  For test 3, the function from test 1
was taken but in the function body a single call to +malloc+ and +free+ was
added.  Test 4 is the same as test 3, but also adds a call to lock and unlock
a mutex.  Test 5 replaces the mutex by a single atomic operation
(compare-and-swap).

As a way to study the effect of the malloc implementation, these tests were
also executed using a +tcmalloc+, an alternative memory allocator library.
The results for these are shown in the tcmalloc column.

[TIP]
The progam used to obtain these results is in the repository:
+tests/blocking-overhead.ae+.



//=========================================================================
== Case study: Implementing a small network server
//=========================================================================

While micro-benchmarks can be useful, they often fail to capture the
complexity found in real applications.

To provide a higher level evaluation, we compared the performance of a simple
network server programmed in aesop to that of the same server implemented in
C. We also quantified the code complexity of the aesop server, compared to the
different C versions.

=== Server Description

The example server listens on a TCP socket for incoming client connections.
Once a client connects, the server waits until a request is received or until
the client closes the connection.

The server recognizes four different request types:

*READ and WRITE*:: The server reads or writes a file specified by the client.

*READ-NULL and WRITE-NULL*:: The same as READ and WRITE respectively, except
that the file read or write operations are omitted. The requested data
is still transferred over the network.

The read-null and write-null cases attempt to determine the maximum network
bandwidth the server can sustain, by ensuring the disk operations are not a
bottleneck.
==== Implementation Details

The server as described above was implemented in 5 different ways.

===== Explicit Threading

The threaded server uses manual thread management to explicitly created or
destroy a thread in response to an incoming connection or request.
We used the `pthread` threading library.
For the threaded implementation, we distinguish between 4 different
variations.


[width="80%",cols=">.^1h,4",frame="none",grid="none"]
|=========================================================
| thread-per-client |
A thread is created when a client connects, and this
thread is dedicated to the connection. All requests from this connection will
be handled by the same thread.

| |


| thread-per-client-nb |
The same as above, but in this case the thread calls
the _asynchronous_ versions of the `read` and `write` system calls. By
including this option, performance differences related to the asynchronous
nature of the system calls are highlighted.

| |

| thread-per-op |
In this mode, a thread is created for every incoming
request. After the request is completed, the thread is destroyed.

| |

|thread-pool | 
As above, but when a thread has finished executing a request
it is returned to a pool and reused when a new request arrives.

|=============================================================

===== Explicit Event Handling

For this version, _libev_ is used to implement an event-driven server.
At any given time, a number of different events can occur, the event loop
waits for one of the following events to occur:

[horizontal]
*Accept*:: An new client connected to the server. The server will start an
attempt to read a request from the connection.
*Read*:: A read from a connection completed.
*Write*:: A write operation (writing data to a client) completed.

In response to one of these events, the next step in handling the connection
will be started using an asynchronous call before going back to the main event
loop to wait for another event to occur.

Note that this server implementation does not use create any threads, and only
utilizes a single core (though the operating system can still use multiple
cores to drive the network and disk).

==== Aesop

The aesop version, from a code point of view, most closely resembles the
_thread_per_client_ code. However, after translation, the resulting C code can
support the event model as well as a threaded model.

The actual result will depend on the actual resource implementation.
(For more information about resources, see the aesop user guide).
It is important to point out that the choice between an event driven or
threaded approach is limited to the resource implementation, and that no
changes to the actual server code are required.

The aesop code uses a lonely pbranch when a client connects.
The code within the lonely pbranch is a direct implementation of the flow
chart provided with the server description.

=== Code Complexity

As a measure for productivity, we investigated the code of each server using a
set of complexity metrics.

.Implementation complexity analysis.
[[table-complex]]
[cols="3,1,1,1", options="header"]
|============================
| Server Implementation | CC | Mod. CC | SLOC 
| aesop | 16 | 11 | 179 
| thread-per-client | 17 | 12 | 182
| thread-per-client-nb | 17 | 12 | 184
| thread-per-op | 22 | 17 | 249
| thread-pool | 32 | 26 | 313
| event | 28 | 23 | 341
|============================

//////
\begin{table}
\small
\begin{center}
\caption{Complexity analysis for example servers}
\begin{tabular}{lrrr}
\hline
& CC & mod. CC & SLOC \\
\hline
\hline
\end{tabular}
\label{tab:complexity}
\end{center}
\normalsize
\vspace{-.2in}
\end{table}
/////

<<table-complex>> compares the code complexity of each server
implementation using McCabe Cyclomatic Complexity (CC) <<McCabe>>,
Modified McCabe Cyclomatic Complexity (Mod. CC), and Source Lines of Code
(SLOC).  The CC and Mod. CC metrics were measured using the pmccabe tool,
version 2.6, created by Paul Bame <<Bame>>, while the SLOC metrics were
measured using the sloccount tool, version 2.26,
created by David A. Wheeler <<Wheeler>>.

To simplify the comparison, error handling was excluded for all servers
except for assertions on expected return codes.  The protocol definition
(ie, request and acknowledgement structs) as well as helper functions to
loop over send and receive were not counted in any of the implementations,
as these were similar in all four.  Also note that the Aesop version
does not include the Aesop standard library, which 
provides a binding between Aesop and the standard POSIX socket API as part
of its default functionality.  The focus
of this comparison is on the core logic defining the server implementation.

The Aesop and thread-per-client servers are very similar in terms of complexity.  The
slight increase in complexity for the thread-per-client server results from
the additional function calls needed to create and join threads.  The
nonblocking version of the thread-per-client server (thread-per-client-nb)
uses two additional lines of code to place each socket into non-blocking
mode.  The remaining code logic needed to manage the nonblocking socket
calls is implemented in helper functions which are not included in the
analysis.

The thread-pool and event models are both much more complex than the
thread-per-client or aesop model.  In the case of the thread-pool server,
this additional complexity arises from not only the queueing and thread
management logic, but also the event loop which is necessary to detect
incoming requests and dispatch them to the queue.  The event server
complexity arises from the necessity of dividing servicing routines into multiple
sub-functions and manually tracking state between those functions.
An additional complexity of the event
model which is not captured by these metrics is the fact that control flow
is not preserved across the processing of a given request.  For example,
servicing a write operation requires 5 disconnected event handlers.
Although the event model appears less complex than the thread-pool model according to CC and
Mod. CC, qualitatively it is significantly more challenging to develop.

=== Performance Evaluation

We evaluated the performance of our server implementation.
The same client was used for all server implementations.

.Performance writing to server memory.
[[fig-write]]
image::fig/write-hist.png["Write"]

.Performance reading from server memory.
[[fig-read]]
image::fig/read-hist.png["Read"]

.Performance writing to disk.
[[fig-write-null]]
image::fig/write-null-hist.png["Writing to disk"]

.Performance reading from disk.
[[fig-read-null]]
image::fig/read-null-hist.png["Reading from disk"]

.Memory usage during the write test
[[fig-write-mem]]
image::fig/write-mem.png["Write test server memory usage"]

.Memory usage during the read test
[[fig-read-mem]]
image::fig/read-mem.png["Read test server memory usage"]

.Memory usage during the write-null test
[[fig-write-mem]]
image::fig/write-null-mem.png["Write-null test server memory usage"]

.Memory usage during the read-null test
[[fig-read-mem]]
image::fig/read-null-mem.png["Read-null test server memory usage"]

== Runtime Memory Efficiency

This section examines the memory efficiency of the translated aesop code.

=== Function arguments and stack variables of blocking calls

Aesop, in order to implement the additional functionality provided by blocking
calls, rewrites blocking calls when translating the aesop code to C code.
This translation introduces a certain amount of overhead, both in memory usage
and execution performance. This section focuses on memory overhead,
deferring the discussion of execution overhead to <<ref-blocking-runtime>>.

The main memory overhead incurred by blocking functions originates from the
need to protect the logical state of the function while temporarily switching
to other functions.

For example, stack variables are moved to the heap. As long as the blocking
function does not complete, the memory for these variables is not released.
Arguments to the function need to be relocated to the heap as well,
and so does the type returned from the function (if not void).

In a normal C program, the items listed above consume stack space. In blocking
functions, these consume heap space instead.  In addition, aesop internally
maintains a number of control structures. Pointers to these structures are
passed as function arguments to the blocking function, and consequently
consume stack space. Currently, aesop adds about 4 pointers and 2 integers to
each blocking function call.

=== Lonely pbranches

A lonely pbranch will keep the enclosing scope alive (up to the function
scope) until the pbranch exits.

.lonely pbranch scope
[source, C]
----
__blocking int test (void)
{
   int var[10000];
   pbranch {
      ...
   }
}
----

So, in the example above, even though the test will return without waiting for
the pbranch to complete, it's stack variables (`var` in this case) will
consume memory until the pbranch returns.




== Compile Time Performance

=== Parallel Make

It is possible to speed up the translation of aesop files by using the `-jn`
option to make, replacing `n` by the desired number of concurrent jobs.

=== CCache

At this time, there are a number of issues blocking the use  of ccache in
combination with the aesop source to source translator (either to cache the
translation or to cache the compilation of the generated C source code).

A first issue is related to incorrect handling of compiler names in the build
system, causing the build to fail if the compiler is set to `ccache gcc`.

The second issue stems from the fact that aesop introduces additional
dependencies which are not understood by ccache. Therefore, subtly failures
would be introduced when aesop is updated and the cache is not manually
cleared.

Given these issues, at this point it is not recommended to use ccache in
combination with aesop. However, both issues can be resolved in a later
aesop release (see ticket #137 and #200 in the triton repository).




== Bibliography

[bibliography]
- [[[McCabe]]] McCabe, T.J. A Complexity Measure. In IEEE Transactions on
  Software Engineering, vol.SE-2, no.4, pp. 308- 320, Dec. 1976.
- [[[Bame]]] Paul Bame.  pmccabe.  http://parisc-linux.org/~bame/pmccabe/
- [[[Wheeler]]] David A. Wheeler.  sloccount.  http://www.dwheeler.com/sloc/
- [[[libev]]] Marc Lehmann. http://software.schmorp.de/pkg/libev.html
